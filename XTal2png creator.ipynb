{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686177f0-7bb0-4d97-aa11-4ca2a3710b29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mp_api.client import MPRester\n",
    "import pymatgen.core.structure\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985202c-06a0-4391-84af-89ccd269c675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with MPRester(\"jS4ST5fsFePAWwMurwaUh5FRcfXmgdA3\") as mpr:\n",
    "    list_of_available_fields = mpr.summary.available_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237d391-804e-4ce0-b78f-b0e50ae48609",
   "metadata": {},
   "outputs": [],
   "source": [
    "stableMaterialsIDDocs = mpr.summary.search(fields = [\"nsites\", \"energy_above_hull\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8b357-5af5-4050-9578-20b0ebbbb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stableMaterialsIDDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173230cd-abf2-416e-bbb6-04aae653b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stablematerialswithcustomdef=[x for x in stableMaterialsIDDocs if x.energy_above_hull < .1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45285c91-c741-44cd-878b-75cc7b58fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stablematerialswithcustomdef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79241f78-f8c3-47cc-b184-e4202d8096e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsites=[]\n",
    "for x in stablematerialswithcustomdef:\n",
    "    nsites.append(x.nsites) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa511f40-72a8-4a74-b4e0-7b14b8e4eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for x in stablematerialswithcustomdef:\n",
    "   if x.nsites < 52:\n",
    "       i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691b3d7-1f4a-41ef-a87e-ff963371cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163ac09-c484-478b-a78f-870a759256c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(nsites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbe34c-1927-41d9-b69d-34838ea43a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(nsites, bins=10, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Array Data')\n",
    "plt.xlim(0, 150)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba79f9-79ad-41fd-84f8-78a8b9675bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = os.listdir(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01725356-0266-49d7-9180-20daecc6ab33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777361d-8217-43ff-9130-29c9304f507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stableMaterialsIDDocs[0].structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10dc55-b2c0-4d76-8a2a-4109bbc0453d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymatgen.ext.matproj import MPRester\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "dict={}\n",
    "for x in tqdm(stablematerialswithcustomdef):\n",
    "    if not os.path.exists(f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\\\\{x.material_id}\\\\\") and len(x.structure)<=52:\n",
    "        os.makedirs(f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\\\\{x.material_id}\\\\\")\n",
    "    xc = XtalConverter(relax_on_decode=False, save_dir=f\"XTAL\\\\TRY3\\\\{x.material_id}\\\\\", max_sites=52, channels=1)\n",
    "    try:\n",
    "        xc.xtal2png([x.structure], show=False, save=True)\n",
    "        dict[x.material_id]=x.formula_pretty\n",
    "    except Exception as e: \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8eb9e-ba8b-4e90-a01f-2a0c60f68447",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4975ab0-2a57-423c-b9a7-b659f99f30f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymatgen.ext.matproj import MPRester\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "\n",
    "\n",
    "# material_ids = [\"mp-757220\"]\n",
    "xc = XtalConverter(relax_on_decode=False, save_dir=\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\\")\n",
    "    \n",
    "for i in range(len(stableMaterialsIDDocs)):\n",
    "    data = xc.xtal2png([docs[0].structure], show=False, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5268d-c764-4425-8010-29e9ad1483f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def resize_images(directory, size=(64, 64)):\n",
    "    x=1\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".png\"):\n",
    "            try:\n",
    "                img_path = os.path.join(directory, filename)\n",
    "                img = Image.open(img_path)\n",
    "                img_resized = img.resize(size, Image.BOX)\n",
    "                img_resized.save(img_path)\n",
    "                print(f\"{x} done\")\n",
    "                x+=1\n",
    "            except Exception as e:\n",
    "                print (f\"skipping image because of {e}\")\n",
    "\n",
    "# Use the function\n",
    "resize_images('firstDataSetRun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f3d73-32f9-4717-a16b-56979c606f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory_path = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\\"  # Replace with the actual path to your directory\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(directory_path)\n",
    "\n",
    "# Filter files that end with '.json.pkl'\n",
    "pickle_files = [file for file in all_files if file.endswith('.json.pkl')]\n",
    "\n",
    "# Display the list of pickle files\n",
    "print(\"List of pickle files:\")\n",
    "for pickle_file in pickle_files:\n",
    "    print(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210322e-452c-48ca-82ca-8199531ffc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "start = time.time()\n",
    "print(start)\n",
    "\n",
    "for pkl in pickle_files:\n",
    "    pathToCurrPickle = os.path.join(directory_path, pkl)\n",
    "    result_data = pd.DataFrame(columns=['mat_id', 'e_above_hull', 'formula', 'band_gap_ind', 'band_gap_dir', 'dos_ef', 'spg', 'energy_total', 'total_mag', 'nsites'])\n",
    "    \n",
    "    with open(pathToCurrPickle, 'rb') as file:\n",
    "        your_loaded_array = pickle.load(file)\n",
    "    \n",
    "    ii = 0\n",
    "    print(f\"pickle {pathToCurrPickle} loaded\")\n",
    "    \n",
    "    for item in your_loaded_array:\n",
    "        try:\n",
    "            item_data = {\n",
    "                'mat_id': item.data['mat_id'],\n",
    "                'e_above_hull': item.data['e_above_hull'],\n",
    "                'formula': item.data['formula'],\n",
    "                'band_gap_ind': item.data['band_gap_ind'],\n",
    "                'band_gap_dir': item.data['band_gap_dir'],\n",
    "                'dos_ef': item.data['dos_ef'],\n",
    "                'spg': item.data['spg'],\n",
    "                'energy_total': item.data['energy_total'],\n",
    "                'total_mag': item.data['total_mag'],\n",
    "                'nsites': item.data['nsites']\n",
    "            }\n",
    "            \n",
    "            # Create a DataFrame from the item_data dictionary\n",
    "            item_df = pd.DataFrame([item_data])\n",
    "            \n",
    "            # Append the item_df to the data DataFrame\n",
    "            result_data = result_data._append(item_df, ignore_index=True)\n",
    "            if ii % 1000 == 0 or ii < 20:\n",
    "                print(f\"{ii} items processed: {time.time() - start}\")\n",
    "            \n",
    "            ii += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    result_data.to_csv(os.path.join(directory_path, pkl.split('.')[0] + '.csv'), index=False)\n",
    "\n",
    "print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec5236-9960-4462-9246-aa6f00ca69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data.to_csv(os.path.join(directory_path, pkl.split('.')[0] + '.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845405e-895e-46e0-8dbc-9434a82b22f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory_path = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\\"  # Replace with the actual path to your directory\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(directory_path)\n",
    "\n",
    "# Filter files that end with '.json.pkl'\n",
    "pickle_files = [file for file in all_files if file.endswith('.json.pkl')]\n",
    "\n",
    "# Display the list of pickle files\n",
    "print(\"List of pickle files:\")\n",
    "for pickle_file in pickle_files:\n",
    "    print(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90343b-e3d0-40e7-8878-3b650ebf3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dcgat_3_001.json.pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d871a58-e90b-4cd1-a634-89105a4340ac",
   "metadata": {},
   "source": [
    "### Alexandera Image creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2abd2a-b1f1-4835-88e6-f43c60deab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a21cff-0e68-471a-a7ba-ff069a2aefda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "\n",
    "start = time.time()\n",
    "print(start)\n",
    "directorypath = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\\"\n",
    "\n",
    "\n",
    "for pkl in pickle_files[2:]:\n",
    "    with open(os.path.join(directorypath,pkl), 'rb') as file:\n",
    "        pickledArray = pickle.load(file)\n",
    "        print(\"pickle loaded\")\n",
    "    xxxxx=[x.structure for x in pickledArray if x.data['nsites'] <= 52]\n",
    "    xc = XtalConverter(relax_on_decode=False, save_dir=f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\Alexandera\\\\{pkl}\\\\\", max_sites=52, channels=1)\n",
    "    xc.xtal2png(xxxxx, show=False, save=True)\n",
    "#     xc = XtalConverter(relax_on_decode=False, save_dir=f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\Alexandera\\\\{pkl}\\\\{entry.data['mat_id']}\", max_sites=52, channels=1)\n",
    "#     xc.xtal2png(xxxxx, show=False, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55222ac5-66f7-4910-94a4-5acaf3d594f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb104e0c-d6bb-4fd8-8fcc-05ff92cac9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxx=[x.structure for x in pickledArray if x.data['nsites'] <= 52]\n",
    "len(xxxxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04525a-fea9-48c7-b02b-9aa97f354efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = XtalConverter(relax_on_decode=False, save_dir=f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\Alexandera\\\\dcgat_1_002.json.pkl\\\\\", max_sites=52, channels=1)\n",
    "xc.xtal2png(xxxxx, show=False, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc287218-3cab-48c5-8de0-e0279f2516c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(start)\n",
    "directorypath = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\\"\n",
    "\n",
    "pathToCurrPickle = os.path.join(directorypath, pkl)\n",
    "with open(pathToCurrPickle, 'rb') as file:\n",
    "    pickledArray = pickle.load(file)\n",
    "    print(\"pickle loaded\")\n",
    "xx=0\n",
    "for entry in pickledArray:\n",
    "    xc = XtalConverter(relax_on_decode=False, save_dir=f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\Alexandera\\\\{pkl}\\\\{entry.data['mat_id']}\", max_sites=52, channels=1)\n",
    "    xc.xtal2png([entry.structure], show=False, save=True)\n",
    "    xx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f2f87-eb7d-4273-a55b-d91eef3bb007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_empty_folders(directory):\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):\n",
    "        for folder in dirs:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            if not os.listdir(folder_path):  # Check if the folder is empty\n",
    "                os.rmdir(folder_path)\n",
    "                print(f\"Deleted empty folder: {folder_path}\")\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\"\n",
    "\n",
    "# Call the function to delete empty folders\n",
    "delete_empty_folders(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f843fd-26e7-4d9e-877c-7ff1dad73d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(directory_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad63cd2-1477-4054-b24e-d40f9e09e996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\material_properties_TRY3.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Function to combine elements and remove spaces\n",
    "def combine_elements(formula):\n",
    "    # Split the formula into elements\n",
    "    elements = formula.split()\n",
    "\n",
    "    # Combine elements into a string without spaces\n",
    "    combined_formula = \"\".join(elements)\n",
    "\n",
    "    return combined_formula\n",
    "\n",
    "# Apply the function to the 'formula' column and create a new 'formulaFileName' column\n",
    "df['formulaFileName'] = df['composition'].apply(combine_elements)\n",
    "df['formulaFileNameReduced'] = df['composition_reduced'].apply(combine_elements)\n",
    "\n",
    "directoryList=os.listdir('C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3DUMP')\n",
    "justIdentifiers = dict(zip(list(map((lambda x: x.split(\",\")[0]), directoryList)), directoryList))\n",
    "\n",
    "def lookUp(fileNameIndex1):\n",
    "    try:\n",
    "        x = justIdentifiers[fileNameIndex1]\n",
    "        del justIdentifiers[fileNameIndex1]\n",
    "        return x\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def lookUp2(fileNameIndex1):\n",
    "    try:\n",
    "        x = justIdentifiers[fileNameIndex1]\n",
    "        del justIdentifiers[fileNameIndex1]\n",
    "        return x\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "df['FileNameFullTry3'] = df['formulaFileName'].apply(lookUp)\n",
    "# df.loc[df['FileNameFullTry3'].isna()].apply(lookUp2)\n",
    "# Save the DataFrame with the new column to a new CSV file\n",
    "# output_csv_file_path = \"output_file.csv\"  # Replace with your desired output file path\n",
    "# df.to_csv(output_csv_file_path, index=False)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df)\n",
    "print(df['composition_reduced'].head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd04fb3c-aff9-48d2-a93d-8a558a589ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_none_values = df['FileNameFullTry3'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7928a0a-777a-47c6-affe-e68746b9ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_none_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe0d5d-36e3-4489-8d87-818a81c9b628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Starting code\")\n",
    "\n",
    "# Load the CSV files into pandas DataFrames\n",
    "combined_df = pd.read_csv(\"combinedAlexanderaWithFilenames.csv\")\n",
    "material_df = pd.read_csv(\"materialProjectCompleteWithFileNameReference - Copy.csv\")\n",
    "print(\"CSV files loaded\")\n",
    "\n",
    "# Extract sets of formulas for faster lookups\n",
    "combined_formulas = set(combined_df['formula'])\n",
    "material_formulas = set(material_df['formula_pretty'])\n",
    "\n",
    "# Find the intersection of the two sets to get matching entries\n",
    "matching_entries = combined_formulas.intersection(material_formulas)\n",
    "\n",
    "# Print the list of matching entries\n",
    "print(\"Matching Entries:\", list(matching_entries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15888d2-f24e-4b29-ba93-f0f411dc35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(matching_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79b974-fbe1-499a-82b0-7344dc3b85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"combinedAlexanderaWithFilenames.csv\")  # Replace \"your_file.csv\" with the path to your CSV file\n",
    "print(\"Columns of the original DataFrame:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Count the number of rows where the value in the \"energy_above_hull\" column is less than 0.1\n",
    "num_rows_less_than_point_one = (df['e_above_hull'] < 0.1).sum()\n",
    "\n",
    "print(\"Number of rows with a value less than 0.1 in the 'e_above_hull' column:\", num_rows_less_than_point_one)\n",
    "\n",
    "# Create a new DataFrame with entries where 'e_above_hull' is less than 0.1\n",
    "new_df = df[df['e_above_hull'] < 0.1]\n",
    "\n",
    "# Save the new DataFrame as a CSV file\n",
    "new_df.to_csv(\"combinedAlexanderaWithFilenames_ONLYSTABLE.csv\", index=False)\n",
    "\n",
    "print(\"Filtered data saved as 'filtered_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075de6a6-38df-49ed-bc8b-ad5b8e2b6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"combinedAlexanderaWithFilenames_ONLYSTABLE.csv\")\n",
    "\n",
    "# Extract the 'nsites' column\n",
    "nsites_column = df['nsites']\n",
    "\n",
    "# Plot distribution of 'nsites' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=nsites_column, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of nsites Column')\n",
    "plt.xlabel('nsites')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f443d95-5c33-4cc7-bcc2-0771760c2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_less_than_50 = (df['nsites'] < 52).sum()\n",
    "\n",
    "print(\"Number of rows with a value less than 0.1 in the 'e_above_hull' column:\", num_rows_less_than_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fad92-a320-4d38-9aa1-2de08fc2c0b7",
   "metadata": {},
   "source": [
    "### Examination of generated CrysTens images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640991a-0533-417d-a011-8bb8babf7bfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import os \n",
    "\n",
    "# Output directory to save resized images\n",
    "output_dir = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\firstDataSetRun\\\\RESIZED\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for fpath in glob.glob(os.path.join(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\firstDataSetRun\\\\\", \"*.png\")):\n",
    "    with Image.open(fpath) as im:\n",
    "        # Resize the image\n",
    "        resized_im = im.resize((64*5, 64*5), Image.BOX)\n",
    "        # Get the filename from the original file path\n",
    "        filename = os.path.basename(fpath)\n",
    "        # Construct the output file path\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        # Save the resized image\n",
    "        resized_im.save(output_path)\n",
    "        print(\"Resized image saved:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda53d8-d9a3-4ad9-8819-ce7bc5f611a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from PIL import Image\n",
    "for fpath in glob.glob(\"C:\\\\Users\\\\91931\\\\Downloads\\\\NEWEVALIMAGES\\\\NEWEVALIMAGES\\\\image_81.png\"):\n",
    "    with Image.open(fpath) as im:\n",
    "      im = im.resize((64*5, 64*5), Image.BOX)\n",
    "      im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "      print(fpath)\n",
    "      display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e4730-86ad-4911-80b6-ca26829f9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the files\n",
    "directory = \"C:\\\\Users\\\\91931\\\\Documents\\\\Work\\\\Year 4\\\\Dissertation\\\\imagenTestOvernightMarch14\\\\imagenTestMarch14Overnight\\\\\"\n",
    "\n",
    "# Get list of files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Extract numbers from filenames and store them in a list\n",
    "numbers = []\n",
    "for file_name in files:\n",
    "    # Split the filename by \"-\"\n",
    "    parts = file_name.split(\"-\")\n",
    "    # Get the last part which contains the number\n",
    "    last_part = parts[-1]\n",
    "    # Extract the number by removing the \".png\" extension and convert it to an integer\n",
    "    number = int(last_part.split(\".\")[0])\n",
    "    # Append the number to the list\n",
    "    numbers.append(number)\n",
    "\n",
    "# Sort the list of numbers\n",
    "numbers.sort()\n",
    "\n",
    "# Print the sorted list\n",
    "print(numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c3633-94f9-47d9-9ddd-f6fbdf29c639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Directory containing the images\n",
    "directory = \"C:\\\\Users\\\\91931\\\\Documents\\\\Work\\\\Year 4\\\\Dissertation\\\\imagenTestOvernightMarch14\\\\imagenTestMarch14Overnight\\\\\"\n",
    "\n",
    "# Get list of files in the directory\n",
    "files = [item for item in os.listdir(directory) if os.path.isfile(os.path.join(directory, item))]\n",
    "\n",
    "# Extract numbers from filenames and store them in a list\n",
    "numbers = []\n",
    "for file_name in files:\n",
    "    # Split the filename by \"-\"\n",
    "    parts = file_name.split(\"_\")\n",
    "    # Get the last part which contains the number\n",
    "    last_part = parts[-1]\n",
    "    # Extract the number by removing the \".png\" extension and convert it to an integer\n",
    "    number = int(last_part.split(\".\")[0])\n",
    "    # Append the number to the list\n",
    "    numbers.append(number)\n",
    "\n",
    "# Sort the list of numbers\n",
    "numbers.sort()\n",
    "\n",
    "# Directory to save resized images\n",
    "output_path = \"C:\\\\Users\\\\91931\\\\Documents\\\\Work\\\\Year 4\\\\Dissertation\\\\imagenTestOvernightMarch14\\\\imagenTestMarch14Overnight\\\\larger\\\\\"\n",
    "\n",
    "# Iterate over the sorted numbers list\n",
    "for number in numbers:\n",
    "    # Recreate the filename pattern\n",
    "    filename = f\"sample-at-step_{number}.png\"\n",
    "    # Construct the full file path\n",
    "    fpath = os.path.join(directory, filename)\n",
    "    # Open the image\n",
    "    with Image.open(fpath) as im:\n",
    "        # Resize the image\n",
    "        im_resized = im.resize((64 * 5, 64 * 5), Image.BOX)\n",
    "        # Construct the output file path for the resized image\n",
    "        output_filename = f\"resized_{filename}\"\n",
    "        output_path_full = os.path.join(output_path, output_filename)\n",
    "        # Save the resized image with PNG format\n",
    "        im_resized.save(output_path_full, format='PNG')\n",
    "        # Display the image\n",
    "        display(im_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ebbc7-dfa6-42ba-ac6b-71fb63865adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = XtalConverter(save_dir=\"data\") # DFT surrogate relaxation via m3gnet by default\n",
    "data = xc.xtal2png(example_structures, save=True)\n",
    "print(type(data))\n",
    "print(data)\n",
    "relaxed_decoded_structures = xc.png2xtal(data, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2791837-85cf-42b3-aa2e-733d21175451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of `pymatgen.core.structure.Structure` objects\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "import glob \n",
    "from PIL import Image\n",
    "\n",
    "xc = XtalConverter()\n",
    "for fpath in glob.glob(\"C:\\\\Users\\\\91931\\\\Downloads\\\\NEWEVALIMAGES\\\\RightSided\\\\image_9.png\"):\n",
    "    with Image.open(fpath) as im:\n",
    "        grayscale_image = im.convert(\"L\")\n",
    "        im_resized = im.resize((64, 64), Image.BOX)\n",
    "        flipped_image = im_resized.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        display(flipped_image)\n",
    "        arrrr=[flipped_image]\n",
    "        decoded_structures = xc.png2xtal(arrrr, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f4352-87b1-4f7d-8364-9b213fb8ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae18c0-3ad9-4f3b-b332-63f2dbd61a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xtal2png.utils.data import example_structures\n",
    "\n",
    "for x in example_structures:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7506419-f7d9-4c6f-9dd5-a10ce0f94b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from ase.visualize import view\n",
    "\n",
    "aaa = AseAtomsAdaptor()\n",
    "display(view(aaa.get_atoms(decoded_structures), viewer='ngl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af28a1b-de73-42b7-894d-4b157871dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = XtalConverter(save_dir=\"data\", relax_on_decode=False)\n",
    "data = xc.xtal2png(example_structures, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036ed85-dc3f-4c56-ad3b-4f0a91fe8227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Directory containing the image files\n",
    "directory = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\data\\\\\"\n",
    "\n",
    "# Iterate over all image files in the directory\n",
    "for fpath in glob.glob(os.path.join(directory, \"*.png\")):\n",
    "    with Image.open(fpath) as im:\n",
    "        # Resize the image\n",
    "        im_resized = im.resize((64*5, 64*5), Image.BOX)\n",
    "        # Print the file path\n",
    "        print(fpath)\n",
    "        # Display the image\n",
    "        display(im_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89166a76-fb7c-4694-8def-9a4a0c779a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "\n",
    "xc = XtalConverter()\n",
    "# Directory containing the image files\n",
    "directory = \"C:\\\\Users\\\\91931\\\\Documents\\\\Work\\\\Year 4\\\\Dissertation\\\\imagenTestOvernightMarch14\\\\imagenTestMarch14Overnight\\\\\"\n",
    "decoded_structures=[]\n",
    "# Iterate over all image files in the directory\n",
    "for fpath in glob.glob(os.path.join(directory, \"*.png\")):\n",
    "    with Image.open(fpath) as im:\n",
    "        # Convert image to grayscale\n",
    "        # grayscale_image = im.convert(\"L\")\n",
    "        # Convert grayscale image to list\n",
    "        arrrr = [list(im.getdata())]\n",
    "        # Process the image\n",
    "        decoded_structures.append(xc.png2xtal([im], save=False))\n",
    "        # Save the decoded structures or perform any other processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54824651-ba0f-4855-8d17-55a16ff95726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad46d3d-4073-48b1-acb2-8090e0520958",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526c1d7-656a-4783-b5f5-2e6ddba14275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir = os.listdir(\"C:\\\\Users\\\\91931\\\\Documents\\\\Work\\\\Year 4\\\\Dissertation\\\\imagenTestOvernightMarch14\\\\imagenTestMarch14Overnight\\\\\")\n",
    "paths=[]\n",
    "baseDir=\"C:\\\\Users\\\\91931\\\\Documents\\\\Work\\\\Year 4\\\\Dissertation\\\\imagenTestOvernightMarch14\\\\imagenTestMarch14Overnight\\\\\"\n",
    "for x in dir:\n",
    "    paths.append(os.path.join(baseDir, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82258333-5da6-498a-8c97-68b0a5522aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4468e590-b490-43d1-99ae-c3449dc76a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Directory containing the images\n",
    "directory = \"C:\\\\Users\\\\91931\\\\Documents\\\\Work\\\\Year 4\\\\Dissertation\\\\imagenTestOvernightMarch14\\\\imagenTestMarch14Overnight\\\\\"\n",
    "\n",
    "# Get list of files in the directory\n",
    "files = [item for item in os.listdir(directory) if os.path.isfile(os.path.join(directory, item))]\n",
    "\n",
    "# Extract numbers from filenames and store them in a list\n",
    "numbers = []\n",
    "for file_name in files:\n",
    "    # Split the filename by \"-\"\n",
    "    parts = file_name.split(\"_\")\n",
    "    # Get the last part which contains the number\n",
    "    last_part = parts[-1]\n",
    "    # Extract the number by removing the \".png\" extension and convert it to an integer\n",
    "    number = int(last_part.split(\".\")[0])\n",
    "    # Append the number to the list\n",
    "    numbers.append(number)\n",
    "\n",
    "# Sort the list of numbers\n",
    "numbers.sort()\n",
    "paths=[]\n",
    "# Iterate over the sorted numbers list\n",
    "for number in numbers:\n",
    "    # Recreate the filename pattern\n",
    "    filename = f\"sample-at-step_{number}.png\"\n",
    "    # Construct the full file path\n",
    "    fpath = os.path.join(directory, filename)\n",
    "    paths.append(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ce8e8-e1da-47e1-8135-b790bc1d55bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a91ef-f3c7-4ee2-bd2f-b6b080a3e8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161e2dc-d900-431f-8f52-0518e5cdd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "import os\n",
    "# paths=list(reversed(paths))\n",
    "HomePath=\"C:\\\\Users\\\\91931\\\\Downloads\\\\NEWEVALIMAGES\\\\LeftSided\"\n",
    "paths=os.listdir(HomePath)\n",
    "fullpaths=[os.path.join(HomePath, x) for x in paths]\n",
    "decoded_structures=[]\n",
    "xc = XtalConverter()\n",
    "\n",
    "i=0\n",
    "for path in fullpaths:\n",
    "    print(path)\n",
    "    print(i)\n",
    "    with Image.open(path) as im:\n",
    "        grayscale_image = im.convert(\"L\")\n",
    "        xc = XtalConverter()\n",
    "        im_resized = grayscale_image.resize((64, 64), Image.BOX)\n",
    "            # flipped_image = im_resized.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        decoded_structures.append(xc.png2xtal([im_resized], save=True))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ad5ca-2afd-4a90-b60b-fe2e44a646e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipDecodedStructures=zip(decoded_structures, fullpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e497c-7e8f-4e5e-8c76-8c3c5bead1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(decoded_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330af6d-f0fc-440f-8979-b4348926d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_structures[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba71370-8d4e-44ad-9e8e-9928eb9d7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zipDecodedStructures:\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851b920-a192-4141-a30b-65d50ed9fddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##30/54 are not valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6beb8c-a6d4-46d4-ad35-52cc54b7bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Open the CSV file\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\combinedAlexanderaWithFilenames.csv\")\n",
    "\n",
    "# Filter the DataFrame based on the conditions\n",
    "filtered_df = df[(df['e_above_hull'] < .1) & (df['nsites'] < 52)]\n",
    "\n",
    "# Get the list of filenames that meet the conditions\n",
    "filenames_to_keep = filtered_df['filename'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b889c8-a287-42bf-9930-01ebbe2f9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filenames_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2217e2c-2dad-43b4-869f-a0ba43541a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Source directory\n",
    "directory = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaCombinedDump\"\n",
    "\n",
    "# Destination directory\n",
    "destination_directory = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaCombinedOnlyStableandLessThan52\"\n",
    "\n",
    "# Iterate over filenames_to_keep and copy files with tqdm progress bar\n",
    "for file_name in tqdm(filenames_to_keep, desc=\"Copying files\"):\n",
    "    # Get the source file path\n",
    "    source_file_path = os.path.join(directory, file_name)\n",
    "    # Copy the file to the destination directory\n",
    "    shutil.copy(source_file_path, destination_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71af723-789b-4314-8d6d-3d7dd0cc5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaCombinedDump\"\n",
    "files_deleted = 0\n",
    "total_files = len(filenames_to_keep)\n",
    "for index, filename in enumerate(os.listdir(directory), start=1):\n",
    "    if filename not in filenames_to_keep:\n",
    "        os.remove(os.path.join(directory, filename))\n",
    "        files_deleted += 1\n",
    "    # Update progress\n",
    "    progress = index / total_files * 100\n",
    "    print(f\"Progress: {progress:.2f}% - Files deleted: {files_deleted}/{index}\", end='\\r')\n",
    "\n",
    "print(\"\\nDeletion process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44292903-2de8-4cab-bdc9-f4bb2b1ddaad",
   "metadata": {},
   "source": [
    "### Alexandera try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed9f2d-0e7d-4dd3-8c88-06cfc69cd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickled file\n",
    "file_path = r\"C:\\Users\\91931\\~\\diss\\dcgat_3_005.json.pkl\"\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Display the 5th element\n",
    "fifth_element = data[4]  # Python is zero-indexed, so the 5th element is at index 4\n",
    "print(fifth_element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05cb399-9cb4-4681-b701-c3bd528aa3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_element.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44511503-69c5-468d-baba-59805ed7fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the directory containing the pickled files\n",
    "directory = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\"\n",
    "\n",
    "# Define the threshold values for filtering\n",
    "e_above_hull_threshold = 0.1\n",
    "n_sites_threshold = 52\n",
    "\n",
    "# Initialize the Main Array to store filtered entries\n",
    "Main_Array = []\n",
    "\n",
    "# Function to process a single pickled file\n",
    "def process_pickled_file(file_path):\n",
    "    filtered_entries = []\n",
    "    # Load the pickled file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    # Initialize tqdm for progress tracking\n",
    "    progress_bar = tqdm(total=len(data), desc=f\"Processing {os.path.basename(file_path)}\", dynamic_ncols=True)\n",
    "    # Loop through each entry in the pickled data\n",
    "    for entry in data:\n",
    "        # Check if the conditions are met\n",
    "        if entry.data['e_above_hull'] < e_above_hull_threshold and entry.data['nsites'] <= n_sites_threshold:\n",
    "            filtered_entries.append(entry)\n",
    "        # Update tqdm progress\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    return filtered_entries\n",
    "\n",
    "# Find all pickled files starting with \"dcgat\" in the directory\n",
    "pickled_files = glob.glob(os.path.join(directory, \"dcgat*.pkl\"))\n",
    "\n",
    "# Process each pickled file\n",
    "for file_path in pickled_files:\n",
    "    # Get filtered entries from the current file\n",
    "    filtered_entries = process_pickled_file(file_path)\n",
    "    # Extend Main Array with filtered entries\n",
    "    Main_Array.extend(filtered_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec7183-6286-4a81-aeee-900d0d04929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Main_Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949aadc0-555b-4b9f-80e1-8cedf1552f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_ids = [x.data['mat_id'] for x in Main_Array]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ca866-1a61-446c-8dd4-7dba817da105",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_value=\"agm002144344\"\n",
    "index = mat_ids.index(search_value)\n",
    "\n",
    "print(\"Index of\", search_value, \":\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e04237-5baa-47af-a4b4-65a60816d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_Array[10656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1dde7-edd1-42fc-a51f-d0a2c5e47db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xtal2png.core import XtalConverter\n",
    "import os\n",
    "\n",
    "# Initialize a list to store dictionaries\n",
    "data_list = []\n",
    "\n",
    "output_root_dir = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaTryII\"\n",
    "i = 0\n",
    "\n",
    "for entry in Main_Array[35991:]:\n",
    "    # Create folder if not exists\n",
    "    folder_path = os.path.join(output_root_dir, entry.data['mat_id'])\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Convert structure to PNG\n",
    "    xc = XtalConverter(save_dir=folder_path, relax_on_decode=False)\n",
    "    xc.xtal2png([entry.structure], show=False, save=True)    \n",
    "    \n",
    "    # Append data dictionary to the list\n",
    "    data_list.append(entry.data)\n",
    "    \n",
    "    print(i)\n",
    "    i += 1\n",
    "\n",
    "# Now data_list contains the data of each object in Main_Array as dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ad64a-7b7d-4f80-aa4f-e575f66ce30f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_info = []\n",
    "destination = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaTryIIDUMP\"\n",
    "folders = os.listdir(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaTryII\")\n",
    "\n",
    "# Initialize tqdm with the total number of folders\n",
    "with tqdm(total=len(folders), desc='Copying files') as pbar:\n",
    "    for folder in folders:\n",
    "        full_path = os.path.join(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaTryII\", folder)\n",
    "        fileName = os.listdir(full_path)[0]\n",
    "        image_info.append({'mat_id': folder, 'file_name': fileName})\n",
    "        shutil.copy(os.path.join(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaTryII\", folder, fileName), os.path.join(destination, fileName))\n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Create DataFrame from image_info\n",
    "df = pd.DataFrame(image_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4247aa-e395-405e-b0f6-97223d3f85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256dbcb-81e6-44ce-b9fa-dfd9cc971cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df1 and df2 are your two dataframes\n",
    "csv_path = r\"C:\\Users\\91931\\~\\diss\\combinedAlexanderaWithFilenames_ONLYSTABLE.csv\"\n",
    "\n",
    "# Load the DataFrame from the CSV file\n",
    "df2 = pd.read_csv(csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e77ea-3727-4e67-b59b-8e6abf85c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate dataframes based on the condition that material_id columns match\n",
    "merged_df = pd.concat([df.set_index('mat_id'), df2.set_index('mat_id')], axis=1, join='inner').reset_index()\n",
    "\n",
    "# Reset the index to make material_id a column again\n",
    "merged_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdf7ea-b3ad-4f3f-b660-dc5f17e83e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b790697-0429-47d8-8f32-91d2d32b5f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c0e29-6e5a-4ff6-8d7d-8edab685edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05f061-0a08-425c-9070-5b62ddd46539",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\alexanderaOnlYStableWithCorrectFileNames.csv\", index=False)  # Set index=False to exclude row indices from the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea478e2-9105-4efd-9f1e-d055a39a74cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming dfs_list is a list containing the DataFrames you want to concatenate\n",
    "# and col_map is a dictionary mapping column names between DataFrames\n",
    "\n",
    "# Example DataFrames\n",
    "df1 = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\alexanderaOnlYStableWithCorrectFileNames.csv\")\n",
    "df2 = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\materialProjectCompleteWithFileNameReference - Copy.csv\")\n",
    "\n",
    "print(df1.head)\n",
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(df2.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f014694-7c1b-4aee-a56e-2585971f8eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mapping columns between DataFrames\n",
    "col_map = {'energy_above_hull': 'e_above_hull', 'mat_id': 'material_id', 'file_name':'filename', 'formula':'formula_pretty', 'dos_ef': 'dos_ef', }\n",
    "\n",
    "# Concatenating DataFrames with specified column mappings\n",
    "result = pd.concat([df1.rename(columns=col_map), df2], ignore_index=True)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9a114-3cb6-4d41-a4ed-60adc815a056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\materialProjectCompleteWithFileNameReference - Copy.csv\")\n",
    "# Deleting rows based on condition\n",
    "# df = df[~df['formula_pretty'].isin(matching_entries)]\n",
    "duplicates_mask = df.duplicated(subset=['formula_pretty', 'energy_above_hull'], keep=False)\n",
    "\n",
    "# Check if any duplicates exist\n",
    "if duplicates_mask.any():\n",
    "    print(\"There are rows with duplicated values in both 'formula_pretty' and 'another_column'.\")\n",
    "    duplicate_rows = df[duplicates_mask]\n",
    "    print(\"Duplicate rows:\\n\", duplicate_rows)\n",
    "else:\n",
    "    print(\"No rows with duplicated values in both 'formula_pretty' and 'another_column' found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db6022-63a7-40e3-a423-f3e15a27ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39138732-48b6-46ed-a543-9f487d18a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize dictionaries to store training and validation losses\n",
    "Training_Loss = {}\n",
    "Validation_Loss = {}\n",
    "\n",
    "# Read the file\n",
    "with open(\"C:\\\\Users\\\\91931\\\\Downloads\\\\IMAGENMARCH26XTALRUN.txt\", 'r') as file:\n",
    "    # Process each line\n",
    "    for line in file:\n",
    "        # Split the line into tokens\n",
    "        tokens = line.strip().split()\n",
    "        # Check if the first word is \"Training\"\n",
    "        if tokens[0] == \"Training\":\n",
    "            # Add the last item to the Training Loss dictionary\n",
    "            Training_Loss[tokens[4]] = float(tokens[-1])\n",
    "        # Check if the first word is \"Validation\"\n",
    "        elif tokens[0] == \"Validation\":\n",
    "            # Add the last item to the Validation Loss dictionary\n",
    "            Validation_Loss[tokens[4]] = float(tokens[-1])\n",
    "\n",
    "print(len(Training_Loss))\n",
    "# Plotting both on the same graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b2165-fc3c-458f-b33a-b1f6539bb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the data points for plotting\n",
    "sampled_training_loss = dict(list(Training_Loss.items())[:50000])  # Adjust the sampling rate as needed\n",
    "sampled_validation_loss = dict(list(Validation_Loss.items())[:50000])  # Adjust the sampling rate as needed\n",
    "\n",
    "# Plotting both on the same graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.plot(sampled_training_loss.keys(), sampled_training_loss.values(), marker='o', label='Training Loss', color='blue')\n",
    "\n",
    "# Plot Validation Loss\n",
    "plt.plot(sampled_validation_loss.keys(), sampled_validation_loss.values(), marker='o', label='Validation Loss', color='red')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "# plt.grid(True)\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('training_validation_loss_plot.png')\n",
    "\n",
    "# Close the plot to release memory\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f942e41-10dc-45b1-927b-c023a7f6e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample the data points for plotting\n",
    "sampled_training_loss = dict(list(Training_Loss.items())[::100])  # Adjust the sampling rate as needed\n",
    "sampled_validation_loss = dict(list(Validation_Loss.items())[::100])  # Adjust the sampling rate as needed\n",
    "\n",
    "# Convert the dictionaries to pandas DataFrames for Seaborn\n",
    "import pandas as pd\n",
    "df_training_loss = pd.DataFrame(sampled_training_loss.items(), columns=['Epoch', 'Training Loss'])\n",
    "df_validation_loss = pd.DataFrame(sampled_validation_loss.items(), columns=['Epoch', 'Validation Loss'])\n",
    "\n",
    "# Plotting both on the same graph using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Training Loss\n",
    "sns.lineplot(x='Epoch', y='Training Loss', data=df_training_loss, marker='o', label='Training Loss', color='blue')\n",
    "\n",
    "# Plot Validation Loss\n",
    "sns.lineplot(x='Epoch', y='Validation Loss', data=df_validation_loss, marker='o', label='Validation Loss', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Show grid\n",
    "# plt.grid(True)\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('training_validation_loss_plot_using_seaborn.png')\n",
    "\n",
    "# Close the plot to release memory\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea51e30-7f55-4d00-b2a7-b1eb73f5eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.ext.matproj import MPRester\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "\n",
    "\n",
    "# material_ids = [\"mp-757220\"]\n",
    "xc = XtalConverter(relax_on_decode=False, save_dir=\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\\")\n",
    "    \n",
    "for i in example_structures:\n",
    "    data = xc.xtal2png([i], show=False, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea176c-9721-47c5-b4e7-96227ac11667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91714550-0681-41fb-82d4-8ad239412160",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath=\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\V4Ni2Se8,space-group=12,uid=a288.png\"\n",
    "with Image.open(fpath) as im:\n",
    "    # Resize the image\n",
    "    im_resized = im.resize((64 * 5, 64 * 5), Image.BOX)\n",
    "    # Construct the output file path for the resized image\n",
    "    output_filename = f\"resized_V4Ni2Se8,space-group=12,uid=a288.png\"\n",
    "    # Save the resized image with PNG format\n",
    "    im_resized.save(output_filename, format='PNG')\n",
    "    # Display the image\n",
    "    display(im_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535d7e9-0005-4dbe-98a4-861a7d077043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from ase.visualize import view\n",
    "from xtal2png.utils.data import example_structures\n",
    "\n",
    "\n",
    "aaa = AseAtomsAdaptor()\n",
    "[display(view(aaa.get_atoms(s), viewer='ngl')) for s in example_structures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb71547-e052-4c01-828b-8f01247475e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647a216-284a-4c0c-885f-b276308a7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_image_files(root_dir):\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.png')):  # Add more extensions if needed\n",
    "                image_files.append(os.path.join(root, file))\n",
    "    return image_files\n",
    "    \n",
    "def expandImage(fpath, filename):\n",
    "    with Image.open(fpath) as im:\n",
    "        # Resize the image\n",
    "        im_resized = im.resize((64 * 5, 64 * 5), Image.BOX)\n",
    "        # Construct the output file path for the resized image\n",
    "        # Save the resized image with PNG format\n",
    "        im_resized.save(filename, format='PNG')\n",
    "        \n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    root_directory = '/home/tusharsoodd/MoreComprehensiveAnalysisOfImagenResults/'\n",
    "    image_files = get_image_files(root_directory)\n",
    "    for index, imagePath in enumerate(image_files):\n",
    "        expandImage(imagePath, f\"image_{index}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7415230-3e4b-4816-8f03-b06f6601362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read lines from the text file\n",
    "with open(\"C:\\\\Users\\\\91931\\\\Downloads\\\\ClassifierTestRunMOBILENET.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize lists to store loss, accuracy, and validation accuracy\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "val_accuracy_values = []\n",
    "\n",
    "# Define regular expressions to extract loss, accuracy, and validation accuracy\n",
    "loss_pattern = re.compile(r'loss: ([\\d.]+)')\n",
    "accuracy_pattern = re.compile(r'accuracy: ([\\d.]+)')\n",
    "val_accuracy_pattern = re.compile(r'val_accuracy: ([\\d.]+)')\n",
    "\n",
    "# Iterate over each line and extract relevant information\n",
    "for line in lines:\n",
    "    # Extract loss\n",
    "    loss_match = loss_pattern.search(line)\n",
    "    if loss_match:\n",
    "        loss_values.append(float(loss_match.group(1)))\n",
    "    \n",
    "    # Extract accuracy\n",
    "    accuracy_match = accuracy_pattern.search(line)\n",
    "    if accuracy_match:\n",
    "        accuracy_values.append(float(accuracy_match.group(1)))\n",
    "    \n",
    "    # Extract validation accuracy\n",
    "    val_accuracy_match = val_accuracy_pattern.search(line)\n",
    "    if val_accuracy_match:\n",
    "        val_accuracy_values.append(float(val_accuracy_match.group(1)))\n",
    "\n",
    "# Plot loss and validation accuracy\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot loss on primary y-axis\n",
    "ax1.plot(epochs, loss_values, label='Loss', color='tab:red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Create secondary y-axis for accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs, accuracy_values, label='Accuracy', color='tab:blue')\n",
    "ax2.plot(epochs, val_accuracy_values, label='Validation Accuracy', color='tab:green')\n",
    "ax2.set_ylabel('Accuracy', color='tab:blue')\n",
    "ax2.set_ylim(0.5, 1.0)\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50dc43-9b55-4f9e-9867-debc53ac9442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
