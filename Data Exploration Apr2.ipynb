{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc34d4-9cca-4258-97da-229ee3bad7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\alexanderaOnlYStableWithCorrectFileNames.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06ee03-42e3-49fe-85ba-aff5a62ef241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc455bb-29f1-45b8-b010-7f9a39fd94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert DataFrame to dictionary\n",
    "labels_dict = df.set_index('file_name')['class'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49849699-c0ac-4c09-a17a-db5c3a7bfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\combinedAlexanderaWithFilenames_ONLYSTABLE.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df_filtered = df[(df['e_above_hull'] < 100)]\n",
    "\n",
    "# Plot a histogram of the column 'e_above_hull'\n",
    "hist, bin_edges, _ = plt.hist(df_filtered['e_above_hull'], bins=2, color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title('Energy above Hull binned')\n",
    "plt.xlabel('e_above_hull')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot vertical lines for bin edges\n",
    "for edge in bin_edges:\n",
    "    plt.axvline(x=edge, color='red', linestyle='--', alpha=0.7)\n",
    "plt.savefig('e_above_hull_binned_example.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27392ec4-688c-4459-b101-9066323ae699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75997f8-bcb0-4159-88ee-12381e09f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\combinedAlexanderaWithFilenames_ONLYSTABLE.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check for repetitions in the \"formula\" column\n",
    "repeated_formulas = df['formula'].value_counts()[df['formula'].value_counts() > 1]\n",
    "\n",
    "# Print the repeated formulas\n",
    "if not repeated_formulas.empty:\n",
    "    print(\"Repeated formulas:\")\n",
    "    print(repeated_formulas)\n",
    "else:\n",
    "    print(\"No repeated formulas found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf879e59-2c04-45c3-b389-fadf8aa27e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df = df[df['formula'] == 'AlAg']\n",
    "repeated_energies = filtered_df['e_above_hull'].value_counts()[filtered_df['e_above_hull'].value_counts() > 1]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c78c28-72f8-488a-ac25-8afa47b78c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repeated_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a25c4-3e32-4385-ae77-36587f48a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and 'e_above_hull' is the column you want to bin\n",
    "num_bins = 4\n",
    "\n",
    "# Define the bin edges using numpy's linspace function\n",
    "bin_edges = pd.cut(df['e_above_hull'], bins=num_bins, labels=False)\n",
    "print(bin_edges)\n",
    "# Create a new column 'e_above_hull_bins' in the DataFrame to store the bin values\n",
    "df['e_above_hull_bins'] = bin_edges\n",
    "\n",
    "# Optionally, if you want to convert the bin edges to categorical labels instead of numeric bins\n",
    "bin_labels = ['Bin {}'.format(i) for i in range(1, num_bins+1)]\n",
    "df['e_above_hull_bins'] = pd.cut(df['e_above_hull'], bins=num_bins, labels=bin_labels)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418e6f8-29c0-41df-8354-2286baa2834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "# Load the CSV file into a DataFrame\n",
    "# file_path = r\"C:\\Users\\91931\\~\\diss\\materialProjectCompleteWithFileNameReference - Copy.csv\"\n",
    "file_path = r\"C:\\Users\\91931\\~\\diss\\alexanderaOnlYStableWithCorrectFileNames.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "stats = df.describe().transpose()[['mean', 'min', 'max', 'std']]\n",
    "# print(stats)\n",
    "\n",
    "columnName = ['total_mag']\n",
    "skewedColumnName = ['total_mag']\n",
    "lower_bound=np.inf\n",
    "# selected_values = df[(df[columnName] >= lower_bound) & (df[columnName] <= upper_bound)]['column_name']\n",
    "\n",
    "# band_gap_ind', 'band_gap_dir', 'dos_ef', 'energy_total', 'total_mag']\n",
    "\n",
    "# df[skewedColumnName] = np.sqrt(df[skewedColumnName])\n",
    "\n",
    "mean = df[skewedColumnName].mean()\n",
    "std_dev = df[skewedColumnName].std()\n",
    "df[skewedColumnName] = (df[skewedColumnName] - mean) / std_dev\n",
    "\n",
    "# Take square root of the standardized column\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(df)\n",
    "\n",
    "stats = df.describe().transpose()[['mean', 'min', 'max', 'std']]\n",
    "\n",
    "# Print the statistics\n",
    "# print(stats)\n",
    "\n",
    "\n",
    "for columnN in columnName:\n",
    "    print(f\"Column: {columnN}\")\n",
    "    \n",
    "    # Assuming your DataFrame is called df and the column you're interested in is columnN\n",
    "    \n",
    "    # Mean\n",
    "    mean = df[columnN].mean()\n",
    "    \n",
    "    # Median\n",
    "    median = df[columnN].median()\n",
    "    \n",
    "    # Standard deviation\n",
    "    std_dev = df[columnN].std()\n",
    "    \n",
    "    # Variance\n",
    "    variance = df[columnN].var()\n",
    "    \n",
    "    # Range\n",
    "    range_val = df[columnN].max() - df[columnN].min()\n",
    "    \n",
    "    # Pearson's median skewness\n",
    "    pearson_skewness = 3 * (mean - median) / std_dev\n",
    "    \n",
    "    # Quartiles (Q1, Q3)\n",
    "    Q1 = df[columnN].quantile(0.25)\n",
    "    Q3 = df[columnN].quantile(0.75)\n",
    "    \n",
    "    # Interquartile Range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Lower and Upper limits for outliers\n",
    "    min=df[columnN].min()\n",
    "    max=df[columnN].max()\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Number of Outliers\n",
    "    outliers = df[(df[columnN] < lower_limit) | (df[columnN] > upper_limit)]\n",
    "    num_outliers = len(outliers)\n",
    "    \n",
    "    # Percentage of Outliers\n",
    "    percentage_outliers = (num_outliers / len(df)) * 100\n",
    "\n",
    "# Print the results\n",
    "    print(\"Mean:\", mean)\n",
    "    print(\"Median:\", median)\n",
    "    print(\"Standard Deviation:\", std_dev)\n",
    "    print(\"Variance:\", variance)\n",
    "    print(\"Range:\", range_val)\n",
    "    print(\"Pearson's Median Skewness:\", pearson_skewness)\n",
    "    print(\"min:\", min)\n",
    "    print(\"Q1:\", Q1)\n",
    "    print(\"Q3:\", Q3)\n",
    "    print(\"max:\", max)\n",
    "    print(\"IQR:\", IQR)\n",
    "    print(\"Lower Limit for Outliers:\", lower_limit)\n",
    "    print(\"Upper Limit for Outliers:\", upper_limit)\n",
    "    print(\"Number of Outliers:\", num_outliers)\n",
    "    print(\"Percentage of Outliers:\", percentage_outliers)\n",
    "    print(\"\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25185d7b-a44b-4750-9243-0a7b208c7f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic Summary Statistics\n",
    "print(\"Summary Statistics for Numerical Columns:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nSummary Statistics for Categorical Columns:\")\n",
    "print(df.describe(include=['object']))\n",
    "\n",
    "# Missing Values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Unique Values\n",
    "print(\"\\nUnique Values in Each Column:\")\n",
    "for col in df.columns:\n",
    "    print(col, df[col].nunique())\n",
    "\n",
    "# Distribution of Numerical Columns\n",
    "print(\"\\nHistograms for Numerical Columns:\")\n",
    "df.hist(figsize=(10, 8))\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Value Counts for Categorical Columns\n",
    "print(\"\\nValue Counts for Categorical Columns:\")\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    print(df[col].value_counts())\n",
    "\n",
    "# Boxplots for Outliers\n",
    "print(\"\\nBoxplots for Numerical Columns:\")\n",
    "df.boxplot(figsize=(10, 6))\n",
    "plt.show()\n",
    "\n",
    "# Pairplot for Visualization\n",
    "print(\"\\nPairplot for Numerical Columns:\")\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70657e-a2a2-4132-a472-0e0c44e34b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "# file_path = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\alexanderaOnlYStableWithCorrectFileNames.csv\"\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# Specify the column for which you want to create the plots\n",
    "column_name = \"total_mag\"\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": [0.15, 0.85]})\n",
    "\n",
    "# Plot box plot\n",
    "sns.boxplot(df[column_name], orient=\"h\", ax=ax_box, color='orange')\n",
    "\n",
    "# Plot histogram\n",
    "sns.histplot(data=df, x=column_name, ax=ax_hist, color='orange')\n",
    "\n",
    "# Remove x axis name for the boxplot\n",
    "ax_box.set(xlabel='')\n",
    "ax_hist.set_title(f\"Distribution of Total magnetisation with standardisation for MP dataset\")\n",
    "\n",
    "# Save the plots\n",
    "plt.savefig(f\"Alex_plot_of_{column_name}_stand.png\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63629665-bb26-45c6-8954-df99af513b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your pre-trained image regression model\n",
    "model = tf.keras.models.load_model('C:\\\\Users\\\\91931\\\\Downloads\\\\e_above_hull_0_04999998.h5')\n",
    "\n",
    "# Get the weights of the third convolutional layer\n",
    "third_layer_weights = model.layers[0].get_weights()[0]  # Index 4 corresponds to the third convolutional layer (conv2d_2)\n",
    "\n",
    "# Get the number of filters in the third convolutional layer\n",
    "num_filters = third_layer_weights.shape[3]\n",
    "\n",
    "# Create a subplot grid for visualization\n",
    "rows = int(num_filters / 8) + 1\n",
    "cols = 8\n",
    "\n",
    "plt.figure(figsize=(cols, rows))\n",
    "for i in range(num_filters):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(third_layer_weights[:, :, 0, i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3126b-1d99-4711-8892-4fb261485502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your pre-trained image regression model\n",
    "model = tf.keras.models.load_model('C:\\\\Users\\\\91931\\\\Downloads\\\\e_above_hull_0_04999998.h5')\n",
    "\n",
    "# Get the weights of the third convolutional layer\n",
    "third_layer_weights = model.layers[2].get_weights()[0]  # Index 4 corresponds to the third convolutional layer (conv2d_2)\n",
    "\n",
    "# Get the number of filters in the third convolutional layer\n",
    "num_filters = third_layer_weights.shape[3]\n",
    "\n",
    "# Create a subplot grid for visualization\n",
    "rows = int(num_filters / 8) + 1\n",
    "cols = 8\n",
    "\n",
    "plt.figure(figsize=(cols, rows))\n",
    "for i in range(num_filters):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(third_layer_weights[:, :, 0, i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c099b9-f299-44d0-9ed8-d64b029fe350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your pre-trained image regression model\n",
    "model = tf.keras.models.load_model('C:\\\\Users\\\\91931\\\\Downloads\\\\e_above_hull_0_04999998.h5')\n",
    "\n",
    "# Get the weights of the third convolutional layer\n",
    "third_layer_weights = model.layers[4].get_weights()[0]  # Index 4 corresponds to the third convolutional layer (conv2d_2)\n",
    "\n",
    "# Get the number of filters in the third convolutional layer\n",
    "num_filters = third_layer_weights.shape[3]\n",
    "\n",
    "# Create a subplot grid for visualization\n",
    "rows = int(num_filters / 8) + 1\n",
    "cols = 8\n",
    "\n",
    "plt.figure(figsize=(cols, rows))\n",
    "for i in range(num_filters):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(third_layer_weights[:, :, 0, i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25c23d-4452-4dea-801c-8c0c4f08f6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your pre-trained image regression model\n",
    "model = tf.keras.models.load_model('C:\\\\Users\\\\91931\\\\Downloads\\\\e_above_hull_0_04999998.h5')\n",
    "\n",
    " #Get the weights of the dense layer\n",
    "dense_weights = model.layers[-2].get_weights()[0]  # Assuming the dense layer is the second-to-last layer in the model\n",
    "\n",
    "# Plot the weights\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(dense_weights, cmap='gray', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Input Neurons')\n",
    "plt.ylabel('Neurons in Dense Layer')\n",
    "plt.title('Visualization of Dense Layer Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a40e4-49e2-4ab8-812b-cbf3bf2db1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of the convolutional layer (assuming it's the first convolutional layer)\n",
    "conv_layer = model.layers[0]  # Assuming the first layer is a Conv2D layer\n",
    "conv_weights = conv_layer.get_weights()[0]\n",
    "\n",
    "# Calculate the average activation across all kernels for each position\n",
    "average_activation = np.mean(conv_weights, axis=3)\n",
    "\n",
    "# Plot the average activation\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(average_activation[:, :, 0], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.title('Average Activation of Convolutional Layer Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a5dd4-d5cd-4f2e-8fdf-db9f6b98feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685844fd-aed6-446b-a79b-f8b06e6aa7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the GradCAM class\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer_name):\n",
    "        self.model = model\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.target_layer = None\n",
    "        self.gradients = None\n",
    "\n",
    "        # Find the target layer within the model\n",
    "        self.find_target_layer()\n",
    "\n",
    "        # Register hook to get gradients\n",
    "        self.hook = self.register_hook()\n",
    "\n",
    "    def find_target_layer(self):\n",
    "        for layer in self.model.children():\n",
    "            if layer.__class__.__name__ == self.target_layer_name:\n",
    "                self.target_layer = layer\n",
    "                break\n",
    "\n",
    "    def register_hook(self):\n",
    "        def hook(module, input, output):\n",
    "            self.gradients = output[0].grad\n",
    "\n",
    "        return self.target_layer.register_forward_hook(hook)\n",
    "\n",
    "    def generate(self, input_image, target_class):\n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Forward pass\n",
    "        output = self.model(input_image)\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "\n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        one_hot_output = torch.zeros_like(output)\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "\n",
    "        # Compute GradCAM\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "        cam = torch.sum(weights * self.target_layer.output[0], dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "\n",
    "        return cam\n",
    "\n",
    "# Load pretrained model\n",
    "model = tf.keras.models.load_model('C:\\\\Users\\\\91931\\\\Downloads\\\\e_above_hull_0_04999998.h5')\n",
    "\n",
    "# Define target layer name\n",
    "target_layer_name = \"layer4\"\n",
    "\n",
    "# Initialize GradCAM\n",
    "gradcam = GradCAM(model, target_layer_name)\n",
    "\n",
    "# Load and preprocess input image\n",
    "input_image = cv2.imread(\"input_image.jpg\")\n",
    "input_image = cv2.resize(input_image, (224, 224))\n",
    "input_tensor = torch.tensor(input_image, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "\n",
    "# Generate activation map for target class\n",
    "activation_map = gradcam.generate(input_tensor, target_class=1)\n",
    "\n",
    "# Convert activation map to numpy array\n",
    "activation_map = activation_map.detach().numpy()[0, 0]\n",
    "\n",
    "# Upsample activation map to input image size\n",
    "activation_map = cv2.resize(activation_map, (input_image.shape[1], input_image.shape[0]))\n",
    "\n",
    "# Normalize activation map\n",
    "activation_map = (activation_map - activation_map.min()) / (activation_map.max() - activation_map.min())\n",
    "\n",
    "# Apply colormap\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * activation_map), cv2.COLORMAP_JET)\n",
    "\n",
    "# Overlay heatmap on input image\n",
    "output_image = cv2.addWeighted(input_image, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "# Display and save output image\n",
    "cv2.imshow(\"GradCAM\", output_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite(\"gradcam_output.jpg\", output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd088c6-e2cb-41f7-a259-ba9285ac63bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\materialProjectCompleteWithFileNameReference - Copy.csv\")\n",
    "data['filename'] = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3DUMP\\\\\" + data['filename']\n",
    "\n",
    "# Load and flatten each image\n",
    "image_data = []\n",
    "for i, image_path in enumerate(data['filename'], 1):\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    img = Image.open(image_path)\n",
    "    img_array = np.array(img)\n",
    "    img_array_scaled = img_array / 255.0\n",
    "    img_flat = img_array.ravel()  # Using ravel() for better performance\n",
    "    image_data.append(img_flat)\n",
    "\n",
    "# Convert the list of flattened image data to a numpy array\n",
    "X = np.array(image_data)\n",
    "\n",
    "# Initialize PCA with 2 components for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d91782-0cb6-45b2-a49a-af6dccc2063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the transformed data\n",
    "# Plot the transformed data with smaller dots\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], s=2, c=data['energy_above_hull'], cmap='viridis')\n",
    "plt.colorbar(label='Values')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Visualization of Image Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c73085-c34a-43d5-b673-6061577bfda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA with 3 components for 3D visualization\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the transformed data in 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=data['energy_above_hull'], cmap='viridis')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "plt.title('PCA Visualization of Image Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfab09-93ca-4191-8fb4-af19f74ba27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create a trace\n",
    "trace = go.Scatter3d(\n",
    "    x=X_pca[:, 0],\n",
    "    y=X_pca[:, 1],\n",
    "    z=X_pca[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=data['energy_above_hull'],\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create layout\n",
    "layout = go.Layout(\n",
    "    title='PCA Visualization of Image Data',\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Principal Component 1'),\n",
    "        yaxis=dict(title='Principal Component 2'),\n",
    "        zaxis=dict(title='Principal Component 3')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Show plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cd02a-af17-4321-8ca2-c352a97357b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\materialProjectCompleteWithFileNameReference - Copy.csv\")\n",
    "# Assuming df is your DataFrame\n",
    "non_na_counts = df.count()\n",
    "\n",
    "# Now non_na_counts will contain the number of non-NaN values per column\n",
    "print(non_na_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24606b-9421-4367-81f0-dfdd994ede5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fileName=\"dos_ef_MAPE (1).pkl\"\n",
    "# Load the pickled array\n",
    "file_path = rf\"C:\\Users\\91931\\Downloads\\{fileName}\"\n",
    "with open(file_path, 'rb') as file:\n",
    "    array = pickle.load(file)\n",
    "print(array)\n",
    "# Display the array using matplotlib\n",
    "loss=array['loss']\n",
    "# accuracy=array['accuracy']\n",
    "# val_accuracy=array['val_accuracy']\n",
    "mse=array['mean_squared_error']\n",
    "mae=array['mean_absolute_error']\n",
    "valmse=array['val_mean_squared_error']\n",
    "valrmse=array['val_root_mean_squared_error']\n",
    "rmse=array['root_mean_squared_error']\n",
    "valmae=array['val_mean_absolute_error']\n",
    "valLoss=array['val_loss']\n",
    "# print(val_accuracy)\n",
    "x = list(range(1,len(array['loss'])+1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc0357-0e11-4e63-814c-843b00afb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure and axis objects\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the loss values on the primary (left) y-axis\n",
    "ax1.plot(x, valLoss, 'r-', label='Validation MAPE')\n",
    "ax1.plot(x, loss, 'r--', label='Training MAPE')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('MAPE')  # Set the label and the color of the y-axis to match the line color\n",
    "\n",
    "# Create a second y-axis for the error metrics, sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the error metrics on the secondary (right) y-axis\n",
    "ax2.plot(x, mse, 'b--', label='Train MSE')\n",
    "ax2.plot(x, rmse, 'g--', label='Train RMSE')\n",
    "ax2.plot(x, mae, 'm--', label='Train MAE')\n",
    "ax2.plot(x, valmse, 'b', label='Val MSE')\n",
    "ax2.plot(x, valrmse, 'g', label='Val RMSE')\n",
    "ax2.plot(x, valmae, 'm', label='Val MAE')\n",
    "ax2.set_ylabel('Errors')  # Set the label and the color of the y-axis to match the line color\n",
    "\n",
    "# Get legend handles and labels from both ax1 and ax2\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "\n",
    "# Title of the plot\n",
    "plt.title('Loss and Error Metrics per Epoch of total magnetisation Baseline')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f'plots/Metrics_for_{fileName}.png', bbox_inches='tight')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c947ea4-d8d4-44c9-9ab8-50007d8a709c",
   "metadata": {},
   "source": [
    "# OQMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c8cab-bdcc-4440-ae84-96b850e8018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Open and Load the JSON File\n",
    "with open(\"C:\\\\Users\\\\91931\\\\Downloads\\\\formationenergy.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Step 2: Iterate through the List in the \"data\" Property\n",
    "datadata = data['data']\n",
    "print(len(datadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97fc47-58e9-4478-9cff-5fc4b0268579",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://oqmd.org/oqmdapi/formationenergy?fields=name,entry_id,icsd_id,prototype,ntypes,natoms,volume,delta_e,band_gap,stability&offset=0&sort_offset=0&noduplicate=True&desc=False&filter=stability<0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d261d-1b2f-4a7e-9138-d8638adb654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://oqmd.org/oqmdapi/formationenergy\"\n",
    "params = {\n",
    "    \"filter\": \"stability<0.1\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(type(data))\n",
    "    # Process the data here\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1ccb4-a2d5-42a7-8d38-78fe517be8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b327ad-f201-4269-abdd-e5a35d518cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_all_results(url, params, progress):\n",
    "    print(\"Request sent\")\n",
    "    all_results = []  # Initialize an empty array to store all results\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Response received\")\n",
    "        raw_data = response.json()\n",
    "        progress+=len(raw_data['data'])\n",
    "        print(progress)\n",
    "        all_results.extend(raw_data['data'])  # Add the current page of results to the array\n",
    "        \n",
    "        # Check if there are more pages available\n",
    "        if 'next' in data['links'].keys():\n",
    "            next_url = data['links']['next']\n",
    "            print(next_url)\n",
    "            # Recursively call the function with the next URL\n",
    "            all_results.extend(get_all_results(next_url, params, progress))\n",
    "    \n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257edc99-16fa-4fbb-83ca-0f5528628fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://oqmd.org/oqmdapi/formationenergy\"\n",
    "params = {\n",
    "    \"filter\": \"stability<0.1\",\n",
    "    \"limit\": 1000\n",
    "}\n",
    "\n",
    "all_results = get_all_results(url, params, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f8088-c1b7-44e7-be10-f8b25dfc1818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cacad1-5f97-47db-aa27-898babaf662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean = df[ColumnName].mean()\n",
    "std_dev = df[ColumnName].std()\n",
    "df[ColumnName] = (df[ColumnName] - mean) / std_dev\n",
    "\n",
    "# Take square root of the standardized column\n",
    "df[ColumnName] = np.sqrt(df[ColumnName])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05402240-1df5-467b-9e27-5429f1f5940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the starting image\n",
    "image = cv2.imread(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\ECDDATAEMERGENCY\\\\PLOT_GAUSSIAN_5507_0.png\")\n",
    "\n",
    "# Generate Gaussian noise\n",
    "def add_noise(image, mean=0, sigma=25):\n",
    "    row, col, _ = image.shape\n",
    "    gauss = np.random.normal(mean, sigma, (row, col, 3))\n",
    "    noisy = np.clip(image + gauss, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "# Visualize the progression of noise levels\n",
    "num_levels = 7\n",
    "fig, axes = plt.subplots(1, num_levels+1, figsize=(15, 3))\n",
    "axes[0].axis('off')\n",
    "axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "# axes[0].set_title('Original Image')\n",
    "\n",
    "for i in range(1, num_levels+1):\n",
    "    noisy_image = add_noise(image, sigma=i*175)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].imshow(cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB))\n",
    "    # axes[i].set_title(f'Level {i}')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.savefig(\"noisy_progression_ECD.png\", bbox_inches='tight', pad_inches=0)  # Save the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ab557-7f3b-4cc5-ad80-1b9262c6191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the image (assuming it's a binary image with values of 0 and 1)\n",
    "image = cv2.imread(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\ECDDATAEMERGENCY\\\\PLOT_GAUSSIAN_11762_0.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Count the number of pixels with value 1\n",
    "num_pixels_value_1 = (image == 1).sum()\n",
    "\n",
    "print(\"Number of pixels with value 1:\", num_pixels_value_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa380b4-4e54-48e2-9c5c-0633c8a3ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\ECDDATAEMERGENCY\\\\PLOT_GAUSSIAN_251754_0.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply Gaussian blur to reduce noise\n",
    "blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "# Threshold the image to create a binary image\n",
    "_, thresholded = cv2.threshold(blurred, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Find contours in the binary image\n",
    "contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw contours on the original image\n",
    "image_with_contours = cv2.drawContours(cv2.cvtColor(image, cv2.COLOR_GRAY2BGR), contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# Count the number of detected contours (glows)\n",
    "num_glows = len(contours)\n",
    "print(\"Number of glows:\", num_glows)\n",
    "\n",
    "# Display the image with detected contours\n",
    "cv2.imshow('Image with Contours', image_with_contours)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a943856-aa54-4c50-b7ab-74e5311791b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss_values\n",
    "del mse_values\n",
    "del rmse_values\n",
    "del mae_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e6241-170f-4f7b-af70-32f22c3b07c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open the file\n",
    "with open(\"C:\\\\Users\\\\91931\\\\Downloads\\\\bandgapTest.txt\", 'r') as file:\n",
    "    # Read the lines\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize lists to store the values\n",
    "loss_values = []\n",
    "mse_values = []\n",
    "rmse_values = []\n",
    "mae_values = []\n",
    "\n",
    "# Extract and store the information from each line\n",
    "for line in lines:\n",
    "    # Split the line by whitespace\n",
    "    parts = line.split()\n",
    "    print(parts)\n",
    "    if len(parts)>=16:\n",
    "        # Extract the relevant values\n",
    "        loss_index = parts.index('loss:')\n",
    "        mse_index = parts.index('mean_squared_error:')\n",
    "        rmse_index = parts.index('root_mean_squared_error:')\n",
    "        mae_index = parts.index('mean_absolute_error:')\n",
    "    \n",
    "        loss = float(parts[loss_index + 1])\n",
    "        mse = float(parts[mse_index + 1])\n",
    "        rmse = float(parts[rmse_index + 1])\n",
    "        mae = float(parts[mae_index + 1])\n",
    "    \n",
    "        # Append the extracted values to the lists\n",
    "        loss_values.append(loss)\n",
    "        mse_values.append(mse)\n",
    "        rmse_values.append(rmse)\n",
    "        mae_values.append(mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ab56d-9f74-4243-b6c7-994a182a2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "average_loss = np.mean(loss_values)\n",
    "average_mse = np.mean(mse_values)\n",
    "average_rmse = np.mean(rmse_values)\n",
    "average_mae = np.mean(mae_values)\n",
    "\n",
    "# Print the averages\n",
    "print(\"Average Loss:\", average_loss)\n",
    "print(\"Average MSE:\", average_mse)\n",
    "print(\"Average RMSE:\", average_rmse)\n",
    "print(\"Average MAE:\", average_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab725086-67f2-4b6f-865b-78e85d77d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the values\n",
    "plt.plot(loss_values[40000:], label='Loss')\n",
    "plt.plot(mse_values[40000:], label='Mean Squared Error')\n",
    "plt.plot(rmse_values[40000:], label='Root Mean Squared Error')\n",
    "plt.plot(mae_values[40000:], label='Mean Absolute Error')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8652071-079d-49a7-8078-050505441643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad113d59-bdb5-4e61-bf33-af70af020a2b",
   "metadata": {},
   "source": [
    "## FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a35c23-7f5b-45d8-8a29-6353ecbc293d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96d631-a06e-4567-b890-0779cc9efa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "inception_model = hub.load('https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f2710-161f-4e3b-83ac-da39dbc500fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(image_dir, batch_size=32):\n",
    "    # Load images\n",
    "    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        image_dir,\n",
    "        image_size=(299, 299),  # Match Inception V3 input size\n",
    "        batch_size=batch_size,\n",
    "        label_mode=None)  # We don't need labels\n",
    "\n",
    "    # Normalize images\n",
    "    dataset = dataset.map(lambda x: x / 255.0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1195f-3bc0-4d7d-8ec4-b514cf24687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inception_features(dataset, model):\n",
    "    features = []\n",
    "    for image_batch in dataset:\n",
    "        batch_features = model(image_batch)\n",
    "        features.append(batch_features)\n",
    "        print(\"1 done\")\n",
    "    features = tf.concat(features, axis=0)\n",
    "    print(\"feature creation done\") \n",
    "    return features.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e502d-0e14-4f2f-bcd0-4d398cb05527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c66d40-07eb-4adf-807f-3f37b41dbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_image_dir = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\ECDDATAEMERGENCY\"\n",
    "generated_image_dir = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\ECDDATAEMERGENCY\"\n",
    "\n",
    "# Load and preprocess images\n",
    "real_images_dataset = load_and_preprocess_images(real_image_dir)\n",
    "generated_images_dataset = load_and_preprocess_images(generated_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a0ba7-0ca1-4b85-9a60-6dec5bf9172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_fid(real_features, gen_features):\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = gen_features.mean(axis=0), np.cov(gen_features, rowvar=False)\n",
    "    fid = tfgan.eval.frechet_classifier_distance_from_activations(mu1, sigma1, mu2, sigma2)\n",
    "    return fid\n",
    "\n",
    "# Calculate features\n",
    "real_features = calculate_inception_features(real_images_dataset, inception_model)\n",
    "gen_features = calculate_inception_features(generated_images_dataset, inception_model)\n",
    "\n",
    "# Compute FID score\n",
    "fid_score = calculate_fid(real_features, gen_features)\n",
    "print(f\"FID Score: {fid_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bba01-fe23-4867-92de-28f87f842748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
