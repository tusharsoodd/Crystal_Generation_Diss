{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e623517-d183-4540-90db-f217b2968922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mp_api.client import MPRester\n",
    "from numpy import ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea7389-b97e-434b-bf4e-5cadd3b6dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c25f76-8638-46dc-ba88-c07bbced77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff582643-f6ec-4edd-af40-1c58940fdecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with MPRester(\"jS4ST5fsFePAWwMurwaUh5FRcfXmgdA3\") as mpr:\n",
    "    list_of_available_fields = mpr.summary.available_fields\n",
    "    print(list_of_available_fields)\n",
    "    docs = mpr.summary.search(fields=[\"material_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620e380-943d-4e1a-ae1c-419cf83fda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4566259-f7d6-41e3-aa99-ed78a94f5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stableMPIDS=[]\n",
    "for x in docs:\n",
    "    if x.energy_above_hull < .1 and x.nsites<20:\n",
    "        stableMPIDS.append(x.material_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808979e9-1d8e-4a68-9423-bdba5c773544",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stableMPIDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b6189-6d66-4f97-be34-56dc69a345df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in stableMPIDS:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0e487-fe41-4c5d-90cc-a4a56e5a9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from monty.json import MontyDecoder\n",
    "\n",
    "# Assume 'json_string' is your JSON string\n",
    "json_string = '{\"@module\": \"pymatgen.core.structure\", \"@class\": \"Structure\", \"charge\": null, \"lattice\": {\"matrix\": [[3.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 3.0]], \"a\": 3.0, \"b\": 3.0, \"c\": 3.0, \"alpha\": 90.0, \"beta\": 90.0, \"gamma\": 90.0, \"volume\": 27.0}, \"sites\": [{\"species\": [{\"element\": \"Li\", \"occu\": 1}], \"abc\": [0.0, 0.0, 0.0], \"xyz\": [0.0, 0.0, 0.0], \"label\": \"Li\", \"properties\": {}}]}'\n",
    "\n",
    "# Convert the JSON string to a pymatgen object\n",
    "pymatgen_object = json.loads(json_string, cls=MontyDecoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234331a-7354-4258-af52-f06eafd80ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.ext.matproj import MPRester\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "import json\n",
    "from monty.json import MontyDecoder\n",
    "\n",
    "\n",
    "# material_ids = [\"mp-757220\"]\n",
    "xc = XtalConverter(relax_on_decode=False, save_dir=\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\\")\n",
    "    \n",
    "data = xc.xtal2png([pymatgen_object], show=True, save=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a4af0-55ac-476f-b4db-4fc27b27ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIMISED CODE\n",
    "\n",
    "import os\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "from xtal2png.utils.data import example_structures\n",
    "from xtal2png.core import XtalConverter\n",
    "from monty.json import MontyDecoder\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = \"C:\\\\Users\\\\91931\\\\Downloads\\\\alexandera\"\n",
    "\n",
    "# Initialize an empty list to store the entries\n",
    "all_entries = []\n",
    "\n",
    "# Iterate over all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(directory_path) if f.endswith(\".json\")]\n",
    "for filename in json_files:\n",
    "    print(filename)\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Filter entries and extend the list\n",
    "    entries = [json.loads(json.dumps(entry), cls=MontyDecoder) for entry in data.get('entries', []) if entry.get('@module') == 'pymatgen.entries.computed_entries']\n",
    "    print(len(all_entries))\n",
    "\n",
    "    # Append the entries for this file to the overall list\n",
    "    with open(f'{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump(entries, f)    \n",
    "# Now 'all_entries' contains all entries that start with '@module' from all JSON files in the directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4576cd87-ad40-4ba2-8509-1ef3d44883b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('alexanderaEntriesPymatGenFormat.pkl', 'wb') as f:\n",
    "    pickle.dump(all_entries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4ac4b-b6ed-4e4a-a0d4-a61bfd90d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_folders_with_one_file(directory):\n",
    "    y=0\n",
    "    threeee=[]\n",
    "    # Get the list of all folders in the specified directory\n",
    "    folders = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "\n",
    "    # Iterate through each folder and check the number of files\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        # Check if the folder contains exactly one file\n",
    "        if len(files) == 3 and os.path.isfile(os.path.join(folder_path, files[0])):\n",
    "            print(f\"{folder}\")\n",
    "            threeee.append(folder)\n",
    "            y+=1\n",
    "    return y, threeee\n",
    "# Specify the directory path\n",
    "directory_path = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY2\"\n",
    "\n",
    "# Call the function with the specified directory\n",
    "numFoldersWithThreeFiles, threeee = find_folders_with_one_file(directory_path)\n",
    "print(numFoldersWithThreeFiles)\n",
    "print(threeee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40de07-f682-4839-b802-8067714a037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfImage32x32(image_path):\n",
    "    if os.path.isfile(image_path) and image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        try:\n",
    "            # Open the image using Pillow\n",
    "            img = Image.open(image_path)\n",
    "            \n",
    "            # Check if the image size is 32x32\n",
    "            if img.size == (32, 32): \n",
    "                img.close()\n",
    "                return True\n",
    "            else: return False\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occured: {e}\")\n",
    "            return False\n",
    "    else: return False\n",
    "\n",
    "testPath=\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY2\\\\mp-1002104\\\\La2Pt2,space-group=63,uid=1985.png\"\n",
    "checkIfImage32x32(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c31f0c-3fd8-42eb-b3a1-2a81e3f657eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def delete_non_32x32_images(folder_path):\n",
    "    # Iterate through each entry in the directory\n",
    "    for entry in os.listdir(folder_path):\n",
    "        entry_path = os.path.join(folder_path, entry)\n",
    "\n",
    "        # Check if the entry is a directory\n",
    "        if os.path.isdir(entry_path) and len(os.listdir(folder_path)):\n",
    "            # Recursively process subdirectories\n",
    "            delete_non_32x32_images(entry_path)\n",
    "        elif os.path.isfile(entry_path) and entry.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                # Open the image using Pillow\n",
    "                img = Image.open(entry_path)\n",
    "                \n",
    "                # Check if the image size is 32x32\n",
    "                if img.size != (64, 32):\n",
    "                    # Delete the image if not 32x32\n",
    "                    img.close()\n",
    "                    os.remove(entry_path)\n",
    "                    print(f\"Deleted {entry_path}\")\n",
    "                \n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {entry_path}: {e}\")\n",
    "\n",
    "# def get_processed_name(file):\n",
    "#     name_parts = file.split(\",\")\n",
    "#     return \",\".join(name_parts[:1])\n",
    "def have_same_processed_name(directory):\n",
    "    processed_names = set()\n",
    "    get_processed_name = lambda file: \",\".join(file.split(\",\")[:2])\n",
    "\n",
    "    for file in [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]:\n",
    "        # print(file)\n",
    "        processed_name = get_processed_name(file)\n",
    "        if processed_name in processed_names:\n",
    "            return True\n",
    "        else:\n",
    "            processed_names.add(processed_name)\n",
    "\n",
    "    return False\n",
    "\n",
    "    # Output the result\n",
    "    # print(\"There are two files with the same processed name in the directory.\" if result else \"There are no two files with the same processed name in the directory.\")\n",
    "\n",
    "\n",
    "def delete_extra_32x32_images(folder_path):\n",
    "    # Initialize a counter for 32x32 images\n",
    "    count_32x32 = 0\n",
    "    # Iterate through each entry in the directory\n",
    "    if have_same_processed_name(folder_path):\n",
    "        os.remove(os.path.join(folder_path, os.listdir(folder_path)[0]))\n",
    "        print(f\"deleted: {os.path.join(folder_path, os.listdir(folder_path)[0])}\")\n",
    "        print(f\"Deleted one of the extra 32x32 images in: {folder_path}\")\n",
    "        \n",
    "        # print(\"##################### No duplicates Found #####################\") \n",
    "        \n",
    "\n",
    "    # for entry in os.listdir(folder_path):\n",
    "    #     # print(entry)\n",
    "    #     entry_path = os.path.join(folder_path, entry)\n",
    "    #     print\n",
    "            \n",
    "    #     # # Check if the entry is a directory\n",
    "    #     # if os.path.isdir(entry_path):\n",
    "    #         # Recursively process subdirectories\n",
    "    #         # delete_extra_32x32_images(entry_path)\n",
    "    #         # print(os.path.join(entry_path, os.listdir(folder_path)[1]))\n",
    "    #         print(f\"Deleted one of the extra 32x32 images: {entry_path}\")\n",
    "        # elif os.path.isfile(entry_path) and entry.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        #     try:\n",
    "        #         # Open the image using Pillow\n",
    "        #         if checkIfImage32x32(entry_path):\n",
    "        #             count_32x32+=1\n",
    "        #             # Delete one of the 32x32 images if there are more than two\n",
    "        #             if count_32x32 > 2:\n",
    "        #                 img.close()\n",
    "        #                 os.remove(entry_path)\n",
    "        #                 print(f\"Deleted one of the extra 32x32 images: {entry_path}\")\n",
    "\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(f\"Error processing {entry_path}: {e}\")\n",
    "\n",
    "def process_folders(base_directory, folder_names):\n",
    "    for folder_name in folder_names:\n",
    "        folder_path = os.path.join(base_directory, folder_name)\n",
    "        \n",
    "        # Call the functions with the specified directory\n",
    "        # try:\n",
    "        # delete_non_32x32_images(folder_path)\n",
    "        delete_extra_32x32_images(folder_path)\n",
    "        # except Exception as e:\n",
    "            # print(f\"Error processing folder {folder_path}: {e}\")\n",
    "\n",
    "# Specify the directory path\n",
    "base_directory = r\"C:\\Users\\91931\\~\\diss\\XTAL\\TRY3\"\n",
    "# Subfolders=os.listdir(base_directory)\n",
    "# Specify the list of folder names\n",
    "folders_to_process = os.listdir(base_directory)  # Add your folder names\n",
    "\n",
    "# Call the function with the specified directory and folder names\n",
    "process_folders(base_directory, folders_to_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab50350-f0ae-43a6-842a-2c86baad9789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_folders_with_x_files(directory, x):\n",
    "    y=0\n",
    "    listFF=[]\n",
    "\n",
    "    def count_folders(directory):\n",
    "        folders = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "        return len(folders)\n",
    "    \n",
    "    # Get the list of all folders in the specified directory\n",
    "    folders = [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "\n",
    "    # Iterate through each folder and check the number of files\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        files = os.listdir(folder_path)\n",
    "        folderCount=count_folders(folder_path)\n",
    "        # Check if the folder contains x files\n",
    "        if len(files)-folderCount == x:\n",
    "            listFF.append(folder)\n",
    "            # print(f\"{folder}\")\n",
    "            y+=1\n",
    "    return listFF, y\n",
    "# Specify the directory path\n",
    "directory_path = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\\\\\"\n",
    "\n",
    "# Call the function with the specified directory\n",
    "listFF, numFoldersWithXFiles = find_folders_with_x_files(directory_path,0)\n",
    "print(numFoldersWithXFiles)\n",
    "print(listFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7275bd-706b-4c8a-9760-1b49b5750e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_duplicate_folders(directory):\n",
    "    folder_dict = defaultdict(list)\n",
    "\n",
    "    # Iterate through each entry in the directory\n",
    "    for entry in os.listdir(directory):\n",
    "        entry_path = os.path.join(directory, entry)\n",
    "\n",
    "        # Check if the entry is a directory\n",
    "        if os.path.isdir(entry_path):\n",
    "            # Add the entry to the dictionary based on its name\n",
    "            folder_dict[entry].append(entry_path)\n",
    "\n",
    "    # Filter out folders with unique names\n",
    "    duplicate_folders = {name: paths for name, paths in folder_dict.items() if len(paths) > 1}\n",
    "\n",
    "    # Print folders with the same name\n",
    "    for name, paths in duplicate_folders.items():\n",
    "        print(f\"Folders with the same name '{name}': {paths}\")\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\"\n",
    "\n",
    "# Call the function with the specified directory\n",
    "find_duplicate_folders(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a88e2-995d-4a91-ae8e-95b2daa31ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def count_folders_by_file_count(directory):\n",
    "    print(\"start\")\n",
    "    folder_counts = Counter()\n",
    "\n",
    "    # Iterate through each entry in the directory\n",
    "    for entry in os.listdir(directory):\n",
    "        entry_path = os.path.join(directory, entry)\n",
    "\n",
    "        # Check if the entry is a directory\n",
    "        if os.path.isdir(entry_path):\n",
    "            # Count the number of files in the folder\n",
    "            file_count = len(os.listdir(entry_path))\n",
    "            folder_counts[file_count] += 1\n",
    "\n",
    "    # Print the results\n",
    "    for count, num_folders in folder_counts.items():\n",
    "        print(f\"{num_folders} folders contain {count} files\")\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\"\n",
    "\n",
    "# Call the function with the specified directory\n",
    "count_folders_by_file_count(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33d996-78f1-45af-bd55-5d3ba28bc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del folder_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30268a8a-5889-4322-b157-eb2707dbefcb",
   "metadata": {},
   "source": [
    "### For all folders with no files, add xtal2png files in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49055e-68e0-4c2b-900d-8c869a7bc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=mpr.summary.search(fields=['material_id', 'energy_above_hull', 'structure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05d891-c133-40fb-a40f-2c96bb5c72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacf2c2-0633-4806-9abe-cb8a241ee316",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mp_ids = [x.material_id for x in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63821a5-c831-4eeb-99bb-0ca93983355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mp_ids_copy=all_mp_ids\n",
    "docs_copy=docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df936e02-f4ef-4a46-b399-a6ba809348b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_mp_ids)\n",
    "len(docs_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee23de9-88b2-41c2-af01-def0a5ac1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listFF=os.listdir(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\")\n",
    "len(listFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07b681-c8a9-4c1b-a479-11a737b66d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xtal2png.core import XtalConverter\n",
    "\n",
    "i=0\n",
    "for x in listFF:\n",
    "    entry=docs[all_mp_ids.index(x)]\n",
    "    if not os.path.exists(f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\\\\{x}\\\\\"):\n",
    "        os.makedirs(f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\\\\{x}\\\\\")\n",
    "    xc = XtalConverter(relax_on_decode=False, save_dir=f\"XTAL\\\\TRY3\\\\{x}\\\\\", max_sites=52)\n",
    "    try:\n",
    "        xc.xtal2png([entry.structure], show=False, save=True)\n",
    "        dict[x]=entry.formula_pretty\n",
    "        print(i)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "    i+=1\n",
    "# for x in docs:\n",
    "#     if not os.path.exists(f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY2\\\\{x.material_id}\\\\\"):\n",
    "#         os.makedirs(f\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY2\\\\{x.material_id}\\\\\")\n",
    "#     xc = XtalConverter(relax_on_decode=False, save_dir=f\"XTAL\\\\TRY2\\\\{x.material_id}\\\\\", max_sites=2000)\n",
    "#     try:\n",
    "#         xc.xtal2png([x.structure], show=False, save=True)\n",
    "#         dict[x.material_id]=x.formula_pretty\n",
    "#     except Exception as e: \n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16e8f0-662a-46fc-814c-dfc4993d05e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6fbd58e-7dd9-43e9-9247-995132539b24",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3458857-2155-44ae-8a59-79df536604d2",
   "metadata": {},
   "source": [
    "##### MP Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e41de-546f-4d82-a2ba-325fca9250af",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\"\n",
    "Subfolders = os.listdir(base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed06dd6-3095-4440-9b05-566015b6cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163790ff-b36a-4373-8248-6c63d990e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=mpr.summary.search(fields=['material_id', 'energy_above_hull', 'formula_pretty', 'symmetry', 'is_magnetic', 'energy_per_atom', 'energy_above_hull', 'band_gap', 'space_group', 'nsites', 'elements', 'composition', 'composition_reduced', 'symmetry', 'density', \"cbm\", \"vbm\", \"efermi\", \"dos_energy_up\", \"dos_energy_down\", \"total_magnetization\", \n",
    "\"total_magnetization_normalized_vol\", \"total_magnetization_normalized_formula_units\", \n",
    "\"num_magnetic_sites\", \"num_unique_magnetic_sites\", \"bulk_modulus\", \"shear_modulus\", \n",
    "\"universal_anisotropy\", \"homogeneous_poisson\", \"e_total\", \"e_ionic\", \"e_ij_max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032583f-e885-4b85-b6b3-807881cce43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "data = pd.DataFrame(columns=['mat_id', 'energy_above_hull', 'formula', 'band_gap', 'dos_ef', 'symmetry', 'energy_total', 'nsites', 'elements', 'composition', 'composition_reduced', 'density', \"cbm\", \"vbm\", \"efermi\", \"dos_energy_up\", \"dos_energy_down\", \"total_magnetization\", \n",
    "\"total_magnetization_normalized_vol\", \"total_magnetization_normalized_formula_units\", \n",
    "\"num_magnetic_sites\", \"num_unique_magnetic_sites\", \"bulk_modulus\", \"shear_modulus\", \n",
    "\"universal_anisotropy\", \"homogeneous_poisson\", \"e_total\", \"e_ionic\", \"e_ij_max\"])\n",
    "\n",
    "Subfolders = os.listdir(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\")\n",
    "print(1)\n",
    "Subfolders_set = set(Subfolders)\n",
    "docsFiltered = [thing for thing in docs if thing.material_id in Subfolders_set]\n",
    "\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ade0b9-7128-411f-997b-293470539128",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed=[]\n",
    "for obj in docsFiltered:\n",
    "    try:\n",
    "        data = pd.concat([data, pd.DataFrame({\n",
    "'material_id': [obj.material_id],\n",
    "'energy_above_hull': [obj.energy_above_hull],\n",
    "'formula_pretty': [obj.formula_pretty],\n",
    "'symmetry': [obj.symmetry],\n",
    "'is_magnetic': [obj.total_magnetization],\n",
    "'energy_per_atom': [obj.energy_per_atom],\n",
    "'band_gap': [obj.band_gap],\n",
    "'n_sites': [obj.nsites],\n",
    "'elements': [obj.elements],\n",
    "'composition': [obj.composition],\n",
    "'composition_reduced': [obj.composition_reduced],\n",
    "'density': [obj.density],\n",
    "'cbm': [obj.cbm],\n",
    "'vbm': [obj.vbm],\n",
    "'efermi': [obj.efermi],\n",
    "'dos_energy_up': [obj.dos_energy_up],\n",
    "'dos_energy_down': [obj.dos_energy_down],\n",
    "'total_magnetization': [obj.total_magnetization],\n",
    "'total_magnetization_normalized_vol': [obj.total_magnetization_normalized_vol],\n",
    "'total_magnetization_normalized_formula_units': [obj.total_magnetization_normalized_formula_units],\n",
    "'num_magnetic_sites': [obj.num_magnetic_sites],\n",
    "'num_unique_magnetic_sites': [obj.num_unique_magnetic_sites],\n",
    "'bulk_modulus': [obj.bulk_modulus],\n",
    "'shear_modulus': [obj.shear_modulus],\n",
    "'universal_anisotropy': [obj.universal_anisotropy],\n",
    "'homogeneous_poisson': [obj.homogeneous_poisson],\n",
    "'e_total': [obj.e_total],\n",
    "'e_ionic': [obj.e_ionic],\n",
    "'e_ij_max': [obj.e_ij_max]\n",
    "})], ignore_index=True)\n",
    "        print(f\"{obj.material_id} done\")\n",
    "    except Exception as e:\n",
    "        print(f\"failed for {obj.material_id}: {e}\")\n",
    "        failed.append(obj.material_id)\n",
    "\n",
    "data.to_csv('material_properties_TRY3.csv', index=False)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6d353-5a33-496c-be1a-591f2f8a3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93347f3f-67fc-4f8c-8ee6-61b511d5ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "with MPRester(\"jS4ST5fsFePAWwMurwaUh5FRcfXmgdA3\") as mpr:\n",
    "    list_of_available_fields = mpr.summary.available_fields\n",
    "\n",
    "# with open(\"stablematerialswithcustomdef.pkl\", 'rb') as file:\n",
    "Subfolders = os.listdir(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\")\n",
    "    \n",
    "# base_directory = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY2\"\n",
    "# Subfolders = os.listdir(base_directory)\n",
    "\n",
    "# Initialize an empty DataFrame to store the values\n",
    "data = pd.DataFrame(columns=['mat_id', 'e_above_hull', 'formula', 'band_gap_ind', 'band_gap_dir', 'dos_ef', 'spg', 'energy_total', 'total_mag', 'nsites'])\n",
    "failed=[]\n",
    "# Assuming docs is defined earlier\n",
    "# Initialize MPRester\n",
    "i=0\n",
    "start=time.time()\n",
    "for folder in Subfolders:\n",
    "    try:\n",
    "        # Assuming docs is defined earlier and folder contains the material_id you want to search\n",
    "        ind=all_mp_ids_copy.index(folder)\n",
    "        if ind is None:\n",
    "            print(f\"No index found for: {folder}\")\n",
    "        else:\n",
    "            obj = docs_copy[ind]\n",
    "            data = pd.concat([data, pd.DataFrame({\n",
    "        'material_id': [obj.material_id],\n",
    "        'energy_above_hull': [obj.energy_above_hull],\n",
    "        'formula_pretty': [obj.formula_pretty],\n",
    "        'symmetry': [obj.symmetry],\n",
    "        'is_magnetic': [obj.is_magnetic],\n",
    "        'energy_per_atom': [obj.energy_per_atom],\n",
    "        'band_gap': [obj.band_gap],\n",
    "    })], ignore_index=True)\n",
    "        # Convert the retrieved data into a DataFrame\n",
    "        \n",
    "        # Append the data to the main DataFrame\n",
    "            all_mp_ids_copy.pop(ind)\n",
    "            docs_copy.pop(ind)\n",
    "            if i%500==0:\n",
    "                print(f\"{i} folders done: {time.time()-start}\")\n",
    "            i+=1\n",
    "    except Exception as e:\n",
    "        print(f\"failed for {folder}: {e}\")\n",
    "        failed.append(folder)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data.to_csv('material_properties.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8760866-78a0-4bc6-9827-fc2dc910e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab068c-fbaf-4c6a-b156-2b2269b41cba",
   "metadata": {},
   "source": [
    "### Feature getter for MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19496f9d-1f15-411b-b0a6-77b17d5f3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "with MPRester(\"jS4ST5fsFePAWwMurwaUh5FRcfXmgdA3\") as mpr:\n",
    "    list_of_available_fields = mpr.summary.available_fields\n",
    "\n",
    "Subfolders = os.listdir(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY2\")\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Initialize MPRester\n",
    "i = 0\n",
    "start = time.time()\n",
    "failed = []\n",
    "\n",
    "for folder in Subfolders:\n",
    "    try:\n",
    "        # Assuming docs is defined earlier and folder contains the material_id you want to search\n",
    "        ind = all_mp_ids_copy.index(folder)\n",
    "        if ind is None:\n",
    "            print(f\"No index found for: {folder}\")\n",
    "        else:\n",
    "            obj = docs_copy[ind]\n",
    "            data_list.append({\n",
    "                'material_id': obj.material_id,\n",
    "                'energy_above_hull': obj.energy_above_hull,\n",
    "                'formula_pretty': obj.formula_pretty,\n",
    "                'symmetry': obj.symmetry,\n",
    "                'is_magnetic': obj.is_magnetic,\n",
    "                'energy_per_atom': obj.energy_per_atom,\n",
    "                'band_gap': obj.band_gap,\n",
    "            })\n",
    "            \n",
    "            all_mp_ids_copy.pop(ind)\n",
    "            docs_copy.pop(ind)\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                print(f\"{i} folders done: {time.time()-start}\")\n",
    "            i += 1\n",
    "    except Exception as e:\n",
    "        print(f\"failed for {folder}: {e}\")\n",
    "        failed.append(folder)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "data = pd.DataFrame(data_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data.to_csv('material_properties.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fb2bf-3527-4882-8e98-83220ae15ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelFileFolderLinking(directory):\n",
    "    ListOfFileNamesPerFolderName=[]\n",
    "    folders=os.listdir(directory)\n",
    "    for folder in folders:\n",
    "        ListOfFileNamesPerFolderName.append((folder, os.listdir(os.path.join(directory,folder))[0]))\n",
    "    return ListOfFileNamesPerFolderName\n",
    "\n",
    "\n",
    "directory=\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY2\"\n",
    "listOfFiles = labelFileFolderLinking(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525297a-bc88-4592-8713-68ca0317ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(listOfFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb783df4-8c7f-49ca-a104-a110aa39fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\material_properties.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5228a2b-61bb-4f9c-a3bb-0fd6313880d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea11c59-9f71-47b9-a2d1-16da84a59333",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_id_file_dict = dict(listOfFiles)\n",
    "\n",
    "# Add a new column 'file_name' to the DataFrame based on the 'mp_id' column\n",
    "df['file_name'] = df['material_id'].map(mp_id_file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8952e28-4fc0-46ec-9edd-13ef3c8d768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397d7ac-7e12-47e1-aec3-7d1e01a42936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\material_properties_with_file_names_aka_COMPLETE_MP_REFERENCE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074a0852-ba30-4754-b4c1-3cc18190c1eb",
   "metadata": {},
   "source": [
    "### Code for potentially adding further information in the future for MP database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769ce27-64d9-416d-866d-b4d559bf13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = r\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY2\"\n",
    "Subfolders = os.listdir(base_directory)\n",
    "import pickle\n",
    "with open('mpAllStableMPIDs.pkl', 'wb') as f:\n",
    "    pickle.dump(Subfolders, f)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\material_properties.csv\", encoding='utf-8')\n",
    "\n",
    "with MPRester(\"jS4ST5fsFePAWwMurwaUh5FRcfXmgdA3\") as mpr:\n",
    "    list_of_available_fields = mpr.summary.available_fields\n",
    "\n",
    "jointList=[]\n",
    "for i,subfolder in enumerate(Subfolders):\n",
    "    \n",
    "    docs=mpr.summary.search(material_ids=subfolder, fields=fields+[\"material_id\"])\n",
    "    jointList.append((docs[0].field1, docs[0].field2, docs[0].field3)) \n",
    "    if i%500 == 0:\n",
    "        col1_dict = {t[0]: t[2] for t in listOfFiles}\n",
    "        col2_dict = {t[0]: t[3] for t in listOfFiles}\n",
    "        col3_dict = {t[0]: t[4] for t in listOfFiles}\n",
    "\n",
    "        df['col1'] = df['material_id'].map(col1_dict)\n",
    "        df['col2'] = df['material_id'].map(col2_dict)\n",
    "        df['col3'] = df['material_id'].map(col3_dict)\n",
    "\n",
    "        mp_id_file_dict.clear()\n",
    "        col1_dict.clear()\n",
    "        col2_dict.clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f6fec-7bf2-4d00-8870-f83cb132023f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_images(source_dir, destination_dir):\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Dictionary to store the mapping of folder names to file names\n",
    "    folder_file_mapping = {}\n",
    "\n",
    "    print(\"Starting iterations...\")\n",
    "    # Iterate through each subfolder in the source directory\n",
    "    for foldername in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, foldername)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Get the list of files in the subfolder\n",
    "            files = os.listdir(folder_path)\n",
    "\n",
    "            # Ensure there is exactly one file in the subfolder\n",
    "            if len(files) == 1 and os.path.isfile(os.path.join(folder_path, files[0])):\n",
    "                # Copy the file to the destination directory\n",
    "                source_file = os.path.join(folder_path, files[0])\n",
    "                destination_file = os.path.join(destination_dir, files[0])\n",
    "                shutil.copy(source_file, destination_file)\n",
    "\n",
    "                # Store the mapping in the dictionary\n",
    "                folder_file_mapping[foldername] = files[0]\n",
    "\n",
    "                print(f\"Copied {files[0]} from {foldername} to {destination_dir}\")\n",
    "\n",
    "    return folder_file_mapping\n",
    "\n",
    "# Example usage\n",
    "source_directory = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3\\\\\"\n",
    "destination_directory = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3DUMP\\\\\"\n",
    "\n",
    "fileMapping = copy_images(source_directory, destination_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51309db-6bb7-4ded-8f5d-1b61c6643f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfA = pd.DataFrame(list(fileMapping.items()), columns=['material_id', 'filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b9b28-c2bc-45d9-a944-ad12bfed228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileMappingCopy =fileMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd72d2c-419c-4f9c-81e1-28d16e379699",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = r\"C:\\Users\\91931\\~\\diss\\material_properties_TRY3.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "dfB = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2086a1-4bea-4a22-aabb-d1062b22255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(dfA, dfB, on='material_id', how='left')\n",
    "\n",
    "# Rename the column containing the values from df_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673eb19-8118-4509-ae76-a3c8dc748f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['filename'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807726b8-f5fd-45ae-a3f6-dc4f6c3fb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('materialProjectCompleteWithFileNameReference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738f9ce-1869-432d-9c9e-4db9470a468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = pd.DataFrame(data_dict)\n",
    "\n",
    "# Load CSV into DataFrame (replace 'your_csv_file.csv' with your actual file)\n",
    "csv_file_path = 'your_csv_file.csv'\n",
    "df_csv = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Merge DataFrames based on a common column ('folder' in this case)\n",
    "merged_df = pd.merge(df_csv, df_dict, on='folder', how='left')\n",
    "\n",
    "# Export the merged DataFrame to CSV\n",
    "output_csv_path = 'output_combined.csv'\n",
    "merged_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Merged DataFrame exported to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73275bb6-7253-4ecb-a68a-c41e3ec567ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def count_images_by_dimensions(folder_path):\n",
    "    # Initialize a dictionary to store counts based on dimensions\n",
    "    dimension_counts = {}\n",
    "\n",
    "    # Iterate through each file in the directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        try:\n",
    "            # Open the image file\n",
    "            with Image.open(os.path.join(folder_path, filename)) as img:\n",
    "                # Get the dimensions of the image\n",
    "                width, height = img.size\n",
    "                # Add the dimensions to the dictionary and update the count\n",
    "                dimension_counts[(width, height)] = dimension_counts.get((width, height), 0) + 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {filename}: {str(e)}\")\n",
    "\n",
    "    return dimension_counts\n",
    "\n",
    "# Replace 'folder_path' with the path to your directory containing the images\n",
    "folder_path = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\TRY3DUMP\"\n",
    "image_counts = count_images_by_dimensions(folder_path)\n",
    "\n",
    "# Print the counts\n",
    "for dimensions, count in image_counts.items():\n",
    "    print(f\"Dimensions: {dimensions}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376bd74-574b-41f1-b15e-e8e25f606b8e",
   "metadata": {},
   "source": [
    "### Alexandera Labels and filenames and combined csv creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a521d8-734e-47ec-82ae-773bf7892639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = 'C:\\\\Users\\\\91931\\\\~\\\\diss'\n",
    "\n",
    "# Generator expression to iterate over files in the directory\n",
    "dfs = (pd.read_csv(entry.path).assign(filename=lambda x: x['mat_id'] + '.png')\n",
    "       for entry in os.scandir(directory) if entry.name.startswith('dcgat') and entry.name.endswith('.csv'))\n",
    "\n",
    "print(dfs)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe as a CSV file\n",
    "combined_df.to_csv('combinedAlexanderaWithFilenames.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20638df-36c6-48ee-8af4-ed87b7b360ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('combinedAlexanderaWithFilenames.csv')\n",
    "\n",
    "# Check for any repeated values in the \"filename\" column\n",
    "repeated_filenames = df[df['filename'].duplicated()]\n",
    "\n",
    "if not repeated_filenames.empty:\n",
    "    print(\"Repeated filenames found:\")\n",
    "    print(repeated_filenames)\n",
    "else:\n",
    "    print(\"No repeated filenames found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f2799-9d35-4ff8-b492-5ee594936eda",
   "metadata": {},
   "source": [
    "### Putting all Alexandera images in one folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3caa494-ad56-4758-9ade-5dc490f0fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# Source directory containing folders with images\n",
    "source_directory = 'C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\Alexandera'\n",
    "\n",
    "# Destination directory to dump all images\n",
    "destination_directory = 'C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaCombinedDump'\n",
    "\n",
    "# Find all image files recursively within the source directory\n",
    "image_files = glob.glob(os.path.join(source_directory, '**', '*.png'), recursive=True)\n",
    "\n",
    "print(f\"{len(image_files)} found\")\n",
    "# Copy each image file to the destination directory\n",
    "for image_file in image_files:\n",
    "    shutil.copy(image_file, destination_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc57ea7-cb4f-4369-ad10-9d63321428db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_images_info(directory):\n",
    "    image_info = []\n",
    "    # Initialize tqdm to track progress\n",
    "    print(\"starting loop\")\n",
    "    for dirs in os.listdir(directory):  ## dcgat_1_000.json.pkl\n",
    "        print(dirs)\n",
    "        pbar = tqdm(total=sum(len(files) for files in os.listdir(directory)), desc=f\"{dirs}\")\n",
    "        for folder in os.listdir(os.path.join(source_directory, dirs)):  ## \n",
    "            files = os.listdir(os.path.join(os.path.join(source_directory, dirs), folder))\n",
    "            for file in files:\n",
    "                if file.endswith(('.png')):  # Add more image extensions if needed\n",
    "                    image_info.append({'folder_name': folder, 'file_name': file})\n",
    "                    # Update tqdm progress\n",
    "                    pbar.update(1)\n",
    "    pbar.close()\n",
    "    return image_info\n",
    "\n",
    "# Source directory\n",
    "source_directory = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\Alexandera\"\n",
    "global source_directory\n",
    "print(\"starting processing\")\n",
    "# Extract image info from source directory\n",
    "image_info = extract_images_info(source_directory)\n",
    "\n",
    "# Create DataFrame from image_info\n",
    "df = pd.DataFrame(image_info)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be940e-1af7-483b-8e12-9d9e2ba87225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_images_by_size(folder_path):\n",
    "    # Dictionary to store the count of images for each size\n",
    "    size_counts = {}\n",
    "\n",
    "    # Get the list of image files in the folder\n",
    "    image_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.png')]\n",
    "\n",
    "    # Use tqdm for progress tracking\n",
    "    for filename in tqdm(image_files, desc=\"Processing images\"):\n",
    "        # Get the full path of the image file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Open the image file\n",
    "        with Image.open(file_path) as img:\n",
    "            # Get the size of the image\n",
    "            img_size = img.size\n",
    "            # Increment the count for this size in the dictionary\n",
    "            size_counts[img_size] = size_counts.get(img_size, 0) + 1\n",
    "\n",
    "    return size_counts\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"C:\\\\Users\\\\91931\\\\~\\\\diss\\\\XTAL\\\\AlexanderaTryIIDUMP\"\n",
    "result = count_images_by_size(folder_path)\n",
    "print(\"Number of images for each size:\")\n",
    "for size, count in result.items():\n",
    "    print(f\"Size: {size}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f262870-992f-4483-b9ba-f08e8cea2db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
